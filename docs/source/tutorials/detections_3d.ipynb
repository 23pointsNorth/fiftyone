{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa09791-eedd-4007-8e37-8f736132cd9f",
   "metadata": {},
   "source": [
    "# Evaluating Point-cloud Object Detections with FiftyOne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c4baec-4650-4e7e-9995-590f94ddb7d6",
   "metadata": {},
   "source": [
    "Point-clouds are an efficient way of representing three dimensional spatial data. They can be generated via either laser scanning techniques, such as [Lidar](https://en.wikipedia.org/wiki/Lidar), or photogrammetry, and are used in a variety of computer vision arenas, including autonomous vehicles, robots, and augmented and virtual reality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e9b3a1-d2ed-4778-ae11-334791dbb9ed",
   "metadata": {},
   "source": [
    "This walkthrough demonstrates how to use FiftyOne to work with three dimensional point-cloud data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef37f4b-3ff3-4e4a-970b-256a64e3fe1f",
   "metadata": {},
   "source": [
    "In the walkthrough, we will cover the following topics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b64b9e-39ab-4b5c-9ad6-89c8b4f52be7",
   "metadata": {},
   "source": [
    "* Loading a dataset with point-cloud data into FiftyOne\n",
    "* Visualizing three dimensional data with the FiftyOne $3d$ visualizer\n",
    "* Generating predictions from point-cloud data.\n",
    "* Evaluating three-dimensional object detections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4294e50-7fc0-4327-ab31-902ee0924822",
   "metadata": {},
   "source": [
    "#### So, what's the takeaway?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa25166f-109e-4e9b-b9bf-f0d8489b5f09",
   "metadata": {},
   "source": [
    "While the preprocessing and post-processing of point-cloud data can be quite distinct from the analogous image tasks, the same (or very similar) evaluation and visualization techniques apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2408da0-bfb9-43e7-a754-d7029b450bb3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562b52c7-b0d8-405a-bfbe-ca1e7bcc1c75",
   "metadata": {},
   "source": [
    "If you haven’t already, install FiftyOne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f11cb4-98b9-45d1-a990-d96bb168e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fiftyone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a594ad2-644f-48cf-9450-4a8d60de269e",
   "metadata": {},
   "source": [
    "In this tutorial, we’ll use a Complex-YOLO model in the [FiftyOne Model Zoo](https://voxel51.com/docs/fiftyone/user_guide/model_zoo/index.html). This model is implemented in PyTorch, and to use it, you’ll need to have `torch` and `torchvision` installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede6bffc-7e06-4b2e-b4a2-0a024c041323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aa85ba-6358-4773-98cb-33f442267d40",
   "metadata": {},
   "source": [
    "Now we import all of the packages we will be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60081188-dcdb-4850-aef9-57cd688cb323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import open3d as o3d\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.utils.kitti as fouk\n",
    "import fiftyone.utils.utils_3d as fou3d\n",
    "from fiftyone import ViewField as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e71753b-04c3-4279-9343-3a8ca1fc1655",
   "metadata": {},
   "source": [
    "The data we will be working with is the [KITTI Multiview dataset](https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d), which is conveniently available for download via the [FiftyOne Dataset Zoo](https://voxel51.com/docs/fiftyone/user_guide/dataset_zoo/index.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bded93-9c06-46bc-ac72-e193e9120cac",
   "metadata": {},
   "source": [
    "**NOTE**: In order to robustly capture a three dimensional scene, an individual point-cloud can have hundreds of thousands, millions, or even tens of millions of points. As a result, three dimensional dataset can be quite large. The KITTI Multiview dataset is no exception. It takes up $53.34$GB. All but one section of this tutorial (projecting $3d$ detections into $2d$) work with the [Quickstart Groups dataset](https://voxel51.com/docs/fiftyone/user_guide/dataset_zoo/datasets.html#quickstart-groups), which only requires $516.3$MB of storage. If you want to use the quickstart data instead, switch which of the two lines below is commented out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2044e2f6-3e19-4d90-919d-54447f3b1a8e",
   "metadata": {},
   "source": [
    "Here, we are only going to use the training split of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "607bcfcc-361a-4c19-a063-d2d9899860c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to '/Users/jacobmarks/fiftyone/kitti-multiview/train' if necessary\n",
      "Parsing dataset metadata\n",
      "Found 22443 samples\n",
      "Dataset info written to '/Users/jacobmarks/fiftyone/kitti-multiview/info.json'\n",
      "Loading existing dataset 'pcd_tutorial'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "dataset = foz.load_zoo_dataset(\n",
    "    \"kitti-multiview\",\n",
    "    split=\"train\",\n",
    "    dataset_name=\"pcd_tutorial\"\n",
    ")\n",
    "                              \n",
    "# dataset = foz.load_zoo_dataset(\n",
    "#     \"quickstart-groups\",\n",
    "#     split=\"train\",\n",
    "#     dataset_name=\"pcd_tutorial\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a70a8-782c-4bdf-ac9c-f92109bb9db7",
   "metadata": {},
   "source": [
    "## Understanding the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d35ebfd-86b9-4e72-8624-5e7235aef573",
   "metadata": {},
   "source": [
    "In FiftyOne, the KITTI Multiview dataset is a [Grouped Dataset](https://voxel51.com/docs/fiftyone/user_guide/groups.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b4e1b0d-5850-4d7f-9646-afc48e001c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group\n"
     ]
    }
   ],
   "source": [
    "print(dataset.media_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bde931-b9d5-41d4-80f6-daeedd6beb7a",
   "metadata": {},
   "source": [
    "Each group contains three media files corresponding to the same scene - a \"left\" image, a \"right\" image, and a point cloud \"pcd\" file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89e490b1-fc17-4b97-ad71-ea7afd4a9463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['left', 'right', 'pcd']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.group_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e7e3e-79e1-4c4a-89b8-47d9623d09b1",
   "metadata": {},
   "source": [
    "The default group slice for the group is \"left\", as we can see by inspecting the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "299f28de-6540-459e-afa1-95d67fdda718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        pcd_tutorial\n",
       "Media type:  group\n",
       "Group slice: left\n",
       "Num groups:  7481\n",
       "Persistent:  False\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:           fiftyone.core.fields.ObjectIdField\n",
       "    filepath:     fiftyone.core.fields.StringField\n",
       "    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n",
       "    group:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.groups.Group)\n",
       "    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077768f0-8d31-4741-b380-b2fba93a84f9",
   "metadata": {},
   "source": [
    "This is the group slice whose media will appear in the sample grid when we launch the FiftyOne App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83936c88-ea10-4d73-a295-e73e807f055d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?context=ipython&subscription=6e453f2e-ed05-4564-9962-91b0f78774c0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2bb607bb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673ced56-b07e-4272-9f6a-270b99d398d9",
   "metadata": {},
   "source": [
    "When we click on one of the images in the grid, it pulls up all three samples in the associated group. On the right side of FiftyOne App, we can see the point cloud data visualized in FiftyOne's $3d$ visualizer. We'll inspect these three dimensional plots interactively later on in the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f84d6ac-910f-4771-a966-7efb5fdfc86e",
   "metadata": {},
   "source": [
    "### Point clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df09510a-a62e-4d60-96f4-acd56946c43f",
   "metadata": {},
   "source": [
    "In [the original dataset](https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d), point-clouds, which were generated using [Velodyne LiDAR](https://velodynelidar.com/wp-content/uploads/2019/12/63-9243-Rev-E-VLP-16-User-Manual.pdf), are stored in Velodyne `.bin` files. For convenience and interoperability with most three dimensional geometry and computer vision libraries, point-clouds in FiftyOne's KITTI Multiview dataset are stored in `.pcd` files, in the [PCD file format](https://pointclouds.org/documentation/tutorials/pcd_file_format.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee804fa4-b6b4-42a8-bc38-c7b37cedf6ca",
   "metadata": {},
   "source": [
    "FiftyOne's $3d$ visualizer currently supports only `.pcd` files and [PolyLines](https://voxel51.com/docs/fiftyone/user_guide/using_datasets.html#polylines-and-polygons) objects. If your point-clouds are stored in a different format, you will need to convert it first. Many solutions exist for performing these conversions. For instance, to convert from `.ply`, you can read in a file using open3d's `open3d.io.read_point_cloud()` method, and then write out the resulting object to file with the `open3d.io.write_point_cloud()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c149908d-8105-46f7-9bdb-377f854cc72e",
   "metadata": {},
   "source": [
    "**Note**: It is possible to create point-cloud only datasets in FiftyOne. All FiftyOne SDK functionality that applies to the point-clouds in grouped datasets will apply equally to point-clouds in point-cloud only datasets. However, when you launch the FiftyOne App, you will see that the sample grid does not populate!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b4e14-3026-46c5-b425-d7c54a30eea9",
   "metadata": {},
   "source": [
    "We can inspect the point-clouds more closely using the [open3d geometry](http://www.open3d.org/docs/release/python_api/open3d.geometry.html) library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387f9e45-fc68-487d-83cc-125a73e20dd7",
   "metadata": {},
   "source": [
    "To do this, we need to change the active group slice to the \"pcd\" group slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "341edd36-7641-43ea-88c6-dd214dd61e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.group_slice = \"pcd\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e770872e-9eae-4943-bb3e-03dada4086fc",
   "metadata": {},
   "source": [
    "Then we can select one of the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aac6e70-4e0a-48d5-bc9f-878f10b50c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_sample = dataset.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e0084-c348-4c3e-a151-9f2f32248fff",
   "metadata": {},
   "source": [
    "Now we can verify that this sample is a point-cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e86fa47d-d13c-48fe-8ea1-700f094965df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'point-cloud'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcd_sample.media_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652c802-cb41-43e1-8f6b-ef6e8c3ece5a",
   "metadata": {},
   "source": [
    "**Note**: If we wanted to get the point-cloud sample associated with a given \"left\" or \"right\" sample, we could do so by getting the sample's group id with `group_id = sample.group.id`, using the `get_group()` method, `group = dataset.get_group(group_id)`, and then getting the id of the `pcd` element: `pcd_id = group[\"pcd\"]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a475cdd9-ab83-45d8-974c-c1b2ff9dfd46",
   "metadata": {},
   "source": [
    "Now we can load the `.pcd` file into open3d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a854e773-7eb3-4c95-a280-525bd2a1a5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointCloud with 115384 points.\n"
     ]
    }
   ],
   "source": [
    "pcd_filepath = pcd_sample[\"filepath\"]\n",
    "point_cloud = o3d.io.read_point_cloud(pcd_filepath)\n",
    "print(point_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c1e6cc-2750-4e41-b93b-800ea8ce6826",
   "metadata": {},
   "source": [
    "We can also get aggregate info about the point cloud by inspecting the points in the point-cloud. For instance, we can find the minimum and maximum bounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9c6eb52-5108-4db6-bbde-ed6308de0a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_xyz =  [-71.03600311 -21.10499954  -5.15999985]\n",
      "max_xyz =  [73.03900146 53.79700089  2.67199993]\n"
     ]
    }
   ],
   "source": [
    "pcd_points = np.array(point_cloud.points) ## has shape (Npoints, 3) where 2nd dim is (x, y, z) coords\n",
    "min_xyz = np.amin(pcd_points, axis = 0)\n",
    "max_xyz = np.amax(pcd_points, axis = 0)\n",
    "print(\"min_xyz = \", min_xyz)\n",
    "print(\"max_xyz = \", max_xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc0a8ef-e4b9-4c7c-8ee8-9483170764ce",
   "metadata": {},
   "source": [
    "### $3d$ detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eaeed4-9050-439f-9a89-7b66017b6e5a",
   "metadata": {},
   "source": [
    "In this dataset, samples have detection objects in the ground truth field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c77c615a-a2ed-461c-88d3-6a48deade764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Detections: {\n",
       "    'detections': [\n",
       "        <Detection: {\n",
       "            'id': '636c4a0ae570df8a169571d6',\n",
       "            'attributes': {},\n",
       "            'tags': [],\n",
       "            'label': 'Pedestrian',\n",
       "            'bounding_box': [],\n",
       "            'mask': None,\n",
       "            'confidence': None,\n",
       "            'index': None,\n",
       "            'dimensions': [1.89, 0.48, 1.2],\n",
       "            'location': [1.84, 1.47, 8.41],\n",
       "            'rotation': [0, 0.01, 0],\n",
       "        }>,\n",
       "    ],\n",
       "}>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcd_sample.ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c9c76-767c-43b6-8a4e-f9dda076485e",
   "metadata": {},
   "source": [
    "Each registered detection on the point-cloud sample in a group is accompanied by a detection on the left image and on the right image. Let's see an example of this by getting the left and right images corresponding to this point-cloud sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d67c398-2bfd-4a2d-88b1-9a0f78ad43ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'left': <Sample: {\n",
      "    'id': '636c4a34e570df8a16984680',\n",
      "    'media_type': 'image',\n",
      "    'filepath': '/Users/jacobmarks/fiftyone/kitti-multiview/train/left/000000.png',\n",
      "    'tags': ['train'],\n",
      "    'metadata': <ImageMetadata: {\n",
      "        'size_bytes': 893783,\n",
      "        'mime_type': 'image/png',\n",
      "        'width': 1224,\n",
      "        'height': 370,\n",
      "        'num_channels': 3,\n",
      "    }>,\n",
      "    'group': <Group: {'id': '636c4a0ae570df8a169571d1', 'name': 'left'}>,\n",
      "    'ground_truth': <Detections: {\n",
      "        'detections': [\n",
      "            <Detection: {\n",
      "                'id': '636c4a0ae570df8a169571d5',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'Pedestrian',\n",
      "                'bounding_box': [\n",
      "                    0.5820261437908496,\n",
      "                    0.3864864864864865,\n",
      "                    0.08033496732026148,\n",
      "                    0.4457297297297298,\n",
      "                ],\n",
      "                'mask': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "                'truncated': 0.0,\n",
      "                'occluded': 0,\n",
      "            }>,\n",
      "        ],\n",
      "    }>,\n",
      "}>, 'right': <Sample: {\n",
      "    'id': '636c4a34e570df8a16984683',\n",
      "    'media_type': 'image',\n",
      "    'filepath': '/Users/jacobmarks/fiftyone/kitti-multiview/train/right/000000.png',\n",
      "    'tags': ['train'],\n",
      "    'metadata': <ImageMetadata: {\n",
      "        'size_bytes': 865903,\n",
      "        'mime_type': 'image/png',\n",
      "        'width': 1224,\n",
      "        'height': 370,\n",
      "        'num_channels': 3,\n",
      "    }>,\n",
      "    'group': <Group: {'id': '636c4a0ae570df8a169571d1', 'name': 'right'}>,\n",
      "    'ground_truth': <Detections: {\n",
      "        'detections': [\n",
      "            <Detection: {\n",
      "                'id': '636c4a0ae570df8a169571d7',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'Pedestrian',\n",
      "                'bounding_box': [\n",
      "                    0.5447075709264564,\n",
      "                    0.39016517861437716,\n",
      "                    0.0876132487343676,\n",
      "                    0.4422174553700985,\n",
      "                ],\n",
      "                'mask': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "            }>,\n",
      "        ],\n",
      "    }>,\n",
      "}>, 'pcd': <Sample: {\n",
      "    'id': '636c4a34e570df8a16984684',\n",
      "    'media_type': 'point-cloud',\n",
      "    'filepath': '/Users/jacobmarks/fiftyone/kitti-multiview/train/pcd/000000.pcd',\n",
      "    'tags': ['train'],\n",
      "    'metadata': <Metadata: {'size_bytes': 1846328, 'mime_type': 'application/octet-stream'}>,\n",
      "    'group': <Group: {'id': '636c4a0ae570df8a169571d1', 'name': 'pcd'}>,\n",
      "    'ground_truth': <Detections: {\n",
      "        'detections': [\n",
      "            <Detection: {\n",
      "                'id': '636c4a0ae570df8a169571d6',\n",
      "                'attributes': {},\n",
      "                'tags': [],\n",
      "                'label': 'Pedestrian',\n",
      "                'bounding_box': [],\n",
      "                'mask': None,\n",
      "                'confidence': None,\n",
      "                'index': None,\n",
      "                'dimensions': [1.89, 0.48, 1.2],\n",
      "                'location': [1.84, 1.47, 8.41],\n",
      "                'rotation': [0, 0.01, 0],\n",
      "            }>,\n",
      "        ],\n",
      "    }>,\n",
      "}>}\n"
     ]
    }
   ],
   "source": [
    "dataset.group_slice = \"pcd\"\n",
    "group = dataset.get_group(pcd_sample.group.id)\n",
    "print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0fff2e-1dbf-4ba0-8f72-7fb6efc32216",
   "metadata": {},
   "source": [
    "Here we can see that this scene contains one identified \"Pedestrian\". In the left and right images, the position and shape of the box bounding the detected object is represented with a `bounding_box`. which is stored in `[<top-left-x>, <top-left-y>, <width>, <height>]` format, with units expressed relative to the image dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4a624-50cd-4c68-b9cd-b01981610701",
   "metadata": {},
   "source": [
    "Detections on the point-cloud sample, on the other hand, has `dimensions`, `location`, and `rotation` properties. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef515e6-c6eb-4c76-b6f9-9f3469d9e831",
   "metadata": {},
   "source": [
    "* **Dimensions**: encodes object dimensions $[height, width, length]$ in object coordinates\n",
    "* **Location**: encodes the object center location [x, y, z] in point-cloud coordinates\n",
    "* **Rotation**: encodes the object rotation around [x, y, z] camera axes, in $[-\\pi, \\pi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7215919f-4ee4-47e6-9ce1-f21557b752a2",
   "metadata": {},
   "source": [
    "An important detail is that the coordinates of the images are *not* the same as the $(x, y)$ coordinates of the point clouds. To convert between these coordinate systems, as we will do later on, we need information about the orientations of the lidar sensor and cameras used. For the KITTI Multiview dataset, this is encoded in a set of calibration matrices for each scene, which are stored in a text file in the `kitti-multiview/train/calib` and `kitti-multiview/test/calib` directories. These calibration files are not included in the quickstart groups dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578613d-fb94-4992-afce-58da8349ce8b",
   "metadata": {},
   "source": [
    "### Labels in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea796bc-3976-4ef2-8420-69337643716f",
   "metadata": {},
   "source": [
    "Before proceding to the model, it's also important to get a sense for what objects are present in our dataset. We can aggregate counts for all of the detections by label with the `count_values()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e22a8a9-c62c-42d2-bb63-ada34674b72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Van': 2914,\n",
       " 'Tram': 511,\n",
       " 'Pedestrian': 4487,\n",
       " 'DontCare': 11295,\n",
       " 'Misc': 973,\n",
       " 'Car': 28742,\n",
       " 'Truck': 1094,\n",
       " 'Cyclist': 1627,\n",
       " 'Person_sitting': 222}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count_values(\"ground_truth.detections.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e01a1-5cf2-4432-95c5-97a00ed40369",
   "metadata": {},
   "source": [
    "All of the scenes present in this dataset were captured on or near roads, hence people being labeled as \"Pedestrian\" and \"Cyclist\". The label \"DontCare\" encompasses all detected objects that do not fall into one of the other listed classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b705cc-a158-415b-a8b3-86962682aa10",
   "metadata": {},
   "source": [
    "In what follows, we will restrict our attention to objects in the \"Car\", \"Pedestrian\", and \"Cyclist\" classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6636c8c-220d-48a2-842d-249ddf417888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a424e-ca0e-43c3-977a-44efa78a47f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43610804-6906-4fa4-a442-b5a8330de18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64feb56-b1a1-44c1-a41c-c6a264bfe30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0815ea-3e05-4bfd-8341-4c13ab814460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe62f26-06ab-4cb2-a263-14847f6d58f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bee953-e702-47ab-88ac-5738f5605a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262071cd-c574-435b-892e-addb5c185a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd7418-c58e-49ac-a6ab-9d0518f8cc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ebc408-35b1-43e0-9fa8-3f72630437f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9ea14-7243-4d3b-9ac8-5c298ede32c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c951981d-42a8-4c22-b76e-a63ca72a929c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eff22d-ee14-4007-8306-33c3baab0c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389c5bd-b392-4d2d-95b2-83cedc520a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82ff9a76-5551-4d7a-8cf3-787ec0aa6be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to '/Users/jacobmarks/fiftyone/kitti-multiview/train' if necessary\n",
      "Parsing dataset metadata\n",
      "Found 22443 samples\n",
      "Downloading split 'test' to '/Users/jacobmarks/fiftyone/kitti-multiview/test' if necessary\n",
      "Parsing dataset metadata\n",
      "Found 22554 samples\n",
      "Dataset info written to '/Users/jacobmarks/fiftyone/kitti-multiview/info.json'\n",
      "Loading 'kitti-multiview' split 'train'\n",
      "Importing samples...\n",
      " 100% |█████████████| 22443/22443 [1.3s elapsed, 0s remaining, 17.9K samples/s]         \n",
      "Migrating dataset 'kitti-multiview' to v0.18.0\n",
      "Import complete\n",
      "Loading 'kitti-multiview' split 'test'\n",
      "Importing samples...\n",
      " 100% |█████████████| 22554/22554 [329.9ms elapsed, 0s remaining, 68.4K samples/s]     \n",
      "Migrating dataset '2022.11.25.14.38.14' to v0.18.0\n",
      "Import complete\n",
      "Dataset 'kitti-multiview' created\n"
     ]
    }
   ],
   "source": [
    "dataset = foz.load_zoo_dataset(\"kitti-multiview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe67b12-0e13-4273-96ec-000ead6627f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.group_slice = \"pcd\"\n",
    "pcd_sample = dataset.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd4a9979-352a-4a2e-82b7-02ae9bea282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_view = dataset.match_tags(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb65a4-c285-4e3d-9eea-4850f3da3519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dde273d4-0fc0-4e1b-a0c1-4cc886135f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?context=ipython&subscription=1807c142-d551-463b-a339-c7dcf48d3fdb\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2c88d2220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.group_slice = \"left\"\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "557f4c48-5a95-4088-8f99-6712548c2761",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.match_tags(\"train\")[:200].clone()\n",
    "\n",
    "view = dataset[0:100]\n",
    "### Get rid of right slice because we don't use it at all\n",
    "view.group_slice = \"right\"\n",
    "for group in view.iter_groups():\n",
    "    r = group['right'].id\n",
    "    dataset.delete_samples(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9bb56f-3767-404b-9217-93e93c94ad62",
   "metadata": {},
   "source": [
    "## Generate feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d723146-f51d-4b20-9135-8571b45bb6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Front side (of vehicle) Point Cloud boundary for BEV\n",
    "min_bound = (0, -25, -2.73)\n",
    "max_bound = (50, 25, 1.27)\n",
    "bev_width = 608\n",
    "bev_height = 608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f8bc4e2-36ad-4dc4-be60-ab5d18d83d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.group_slice = \"pcd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a86dfddc-d09d-4e42-8c37-2c9d0a16c5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing samples...\n",
      " 100% |█████████████████| 200/200 [17.9s elapsed, 0s remaining, 11.3 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "fou3d.compute_birds_eye_view_maps(dataset, bev_width, bev_height, min_bound = min_bound, max_bound = max_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78023506-2da0-4e3c-a959-8bcb621d21e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = [\"Car\", \"Pedestrian\", \"Cyclist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebd9a5c1-4e8f-4b06-ad95-3e455b3ee6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_view.group_slice = \"pcd\"\n",
    "height = F(\"dimensions\")[0]\n",
    "exp = F(\"ground_truth.detections\").map(height)\n",
    "heights_dict = {}\n",
    "for c in class_list:\n",
    "    heights_dict[c] = train_view.filter_labels(\n",
    "    \"ground_truth\", F(\"label\") == c).mean(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe996bda-4696-4993-92c1-4b0c0c56cb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5099fe78-0a06-4061-8db4-56d57bfee954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28787cfb-5552-46e7-9e67-513a3c1f9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = fo.launch_app(train_view, auto = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4341240-4acc-4058-91cb-96e5ebccd291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d728a7-2447-49af-893e-caf6f07fe16d",
   "metadata": {},
   "source": [
    "## Apply Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1add97ca-e5b6-4e35-a111-6cccce625e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_thresh, nms_thresh = 0.9, 0.3\n",
    "img_size = bev_height\n",
    "args = {\"img_size\": img_size, \"conf_thresh\": conf_thresh, \"nms_thresh\":nms_thresh}\n",
    "model = foz.load_zoo_model(\"complex-yolo-v3-torch\", args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48831f19-d74b-4f33-a2b6-23234c1aafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_class_heights(heights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9eb4ced-38e4-4b53-a994-953bcf1acad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.group_slice = \"bev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "304f074e-cbfd-4944-a6a9-636102790408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |██████████████████████████████████████████████████████████████████████████████████████| 200/200 [59.4s elapsed, 0s remaining, 3.4 it/s]      \n"
     ]
    }
   ],
   "source": [
    "import fiftyone.utils.complex_yolo as foucy\n",
    "foucy.apply_model(model, dataset, feature_map_field = 'feature_map_filepath', pcd_group_slice = 'pcd', min_bound = min_bound, max_bound = max_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c483f-3dfb-4a42-b76c-f8f8b7580e5d",
   "metadata": {},
   "source": [
    "## Transform coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9bbbbeba-1869-4058-929a-ce714ef8b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calib_path(sample):\n",
    "    sample_path = sample.filepath.split(\"/\")\n",
    "    calib_path = sample_path.copy()\n",
    "    calib_path[-2] = \"calib\"\n",
    "    calib_path = \"/\".join(calib_path)\n",
    "    calib_path = calib_path[:-3] + \"txt\"\n",
    "    return calib_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ae6cf96f-54f9-4bc0-acb1-41be9d12eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_left_from_pcd(pcd_sample):\n",
    "    dataset = pcd_sample._dataset\n",
    "    dataset.group_slice = \"pcd\"\n",
    "    group = dataset.get_group(pcd_sample.group.id)\n",
    "    left_id = group[\"left\"].id\n",
    "    dataset.group_slice = \"left\"\n",
    "    left_sample = dataset[left_id]\n",
    "    return left_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4c4fc9f1-61e8-48b6-b74e-c04cd25321a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corners3d(h, w, l, R, t):\n",
    "    corners3d = np.array(\n",
    "        [\n",
    "            [l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2],\n",
    "            [0, 0, 0, 0, -h, -h, -h, -h],\n",
    "            [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2],\n",
    "        ]\n",
    "    )\n",
    "    return R @ corners3d + t[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f430318b-b53e-441f-bd5c-94dd6cb5e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img2d_shape(img_sample):\n",
    "    if \"metadata\" in sample:\n",
    "        return img_sample.metadata.width, img_sample.metadata.height\n",
    "    else:\n",
    "        h, w = cv2.imread(img_sample.filepath).shape[:2]\n",
    "        return w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1cd6cefc-b9c5-4649-9465-8eb387ab7a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corners3d_to_img_boxes(P, corners3d, img2d_shape):\n",
    "    \"\"\"\n",
    "    :param corners3d: (N, 8, 3) corners in rect coordinate\n",
    "    :return: boxes: (N, 4) [x1, y1, x2, y2] in img2d coords\n",
    "    \"\"\"\n",
    "    corners3d = corners3d\n",
    "    num_boxes = corners3d.shape[0]\n",
    "    corners3d_hom = np.concatenate((corners3d, np.ones((num_boxes, 8, 1))), axis=2)  # (N, 8, 4)\n",
    "    img_pts = np.matmul(corners3d_hom, P.T)  # (N, 8, 3)\n",
    "\n",
    "    x, y = img_pts[:, :, 0] / img_pts[:, :, 2], img_pts[:, :, 1] / img_pts[:, :, 2]\n",
    "    x1, y1 = np.min(x, axis=1).reshape(-1, 1), np.min(y, axis=1).reshape(-1, 1)\n",
    "    x2, y2 = np.max(x, axis=1).reshape(-1, 1), np.max(y, axis=1).reshape(-1, 1)\n",
    "    \n",
    "    boxes = np.concatenate((x1, y1, x2, y2), axis=1)\n",
    "    \n",
    "    boxes[:, 0] = np.clip(boxes[:, 0], 0, img2d_shape[0] - 1)\n",
    "    boxes[:, 1] = np.clip(boxes[:, 1], 0, img2d_shape[1] - 1)\n",
    "    boxes[:, 2] = np.clip(boxes[:, 2], 0, img2d_shape[0] - 1)\n",
    "    boxes[:, 3] = np.clip(boxes[:, 3], 0, img2d_shape[1] - 1)\n",
    "    \n",
    "    boxes[:, 0]/=img2d_shape[0]\n",
    "    boxes[:, 2]/=img2d_shape[0]\n",
    "    boxes[:, 1]/=img2d_shape[1]\n",
    "    boxes[:, 3]/=img2d_shape[1]\n",
    "    \n",
    "    boxes[:, 2] -= boxes[:, 0]\n",
    "    boxes[:, 3] -= boxes[:, 1]\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "5181ddbd-eac7-49ff-827b-5ffcc80487d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pcd_sample_detections_to_image(pcd_sample):\n",
    "    if \"predictions\" not in pcd_sample or len(pcd_sample.predictions.detections) == 0:\n",
    "        return\n",
    "    calib_mats = fouk._load_calibration_matrices(get_calib_path(pcd_sample))\n",
    "    P = calib_mats[\"P2\"]\n",
    "    \n",
    "    left_sample = get_left_from_pcd(pcd_sample)\n",
    "    img2d_shape = get_img2d_shape(left_sample)\n",
    "    \n",
    "    corners3d = []\n",
    "    detections3d = pcd_sample.predictions.detections\n",
    "    \n",
    "    for det3d in detections3d:\n",
    "        h, w, l = det3d[\"dimensions\"]\n",
    "        t = np.array(det3d[\"location\"])\n",
    "        R = fouk._roty(det3d[\"rotation\"][1])\n",
    "        corners3d.append(get_corners3d(h, w, l, R, t).T)\n",
    "    corners3d = np.array(corners3d)\n",
    "    bboxes = corners3d_to_img_boxes(P, corners3d, img2d_shape)\n",
    "    \n",
    "    detections2d = []\n",
    "    for i, det3d in enumerate(detections3d):\n",
    "        new_det2d = fo.Detection(\n",
    "            label=det3d.label,\n",
    "            bounding_box=bboxes[i],\n",
    "            confidence=det3d.confidence,\n",
    "            alpha = det3d.alpha\n",
    "        )\n",
    "        detections2d.append(new_det2d)\n",
    "    \n",
    "    left_sample[\"predictions\"] = fo.Detections(detections=detections2d)\n",
    "    left_sample.save()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "58d1d39b-3585-423e-bbb5-240ead5ad8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_detections_to_images(pcd_samples):\n",
    "    for pcd_sample in pcd_samples:\n",
    "        add_pcd_sample_detections_to_image(pcd_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f66ae23c-d70a-4f47-a9b7-3fdf990b7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.group_slice = \"pcd\"\n",
    "pcd_samples = dataset.select_group_slices(\"pcd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "9667d6e6-911b-4aa0-9ac9-5a0ed64e3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_detections_to_images(pcd_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eead61-c46a-4aff-b6c5-fca79b91a7c7",
   "metadata": {},
   "source": [
    "## Evaluate detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "6f39243b-2827-48dc-b18e-27aab65de0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.group_slice = \"left\"\n",
    "eval_view_2d = dataset.filter_labels(\n",
    "        \"ground_truth\", \n",
    "        F(\"label\").is_in(class_list)\n",
    ")\n",
    "\n",
    "results_2d = eval_view_2d.evaluate_detections(\n",
    "    \"predictions\",\n",
    "    iou = 0.5,\n",
    "    compute_mAP=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "8cacacc4-d3ac-40a6-9106-a2c298e3dbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |█████████████████| 200/200 [649.5ms elapsed, 0s remaining, 307.9 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |█████████████████| 200/200 [955.2ms elapsed, 0s remaining, 196.5 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "dataset.group_slice = \"pcd\"\n",
    "eval_view_3d = dataset.filter_labels(\n",
    "        \"ground_truth\", \n",
    "        F(\"label\").is_in(class_list)\n",
    ")\n",
    "\n",
    "results_3d = eval_view_3d.evaluate_detections(\n",
    "    \"predictions\",\n",
    "    iou = 0.3,\n",
    "    compute_mAP=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "e38866fe-75bc-4a92-9740-9d7ac915b73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Car       0.72      0.70      0.71       733\n",
      "     Cyclist       0.60      0.54      0.57        46\n",
      "  Pedestrian       0.10      0.11      0.10       104\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       883\n",
      "   macro avg       0.47      0.45      0.46       883\n",
      "weighted avg       0.64      0.63      0.63       883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_2d.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ca5987cd-b23a-432b-b89e-6fda90f68848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Car       0.88      0.86      0.87       733\n",
      "     Cyclist       0.62      0.57      0.59        46\n",
      "  Pedestrian       0.34      0.37      0.35       104\n",
      "\n",
      "   micro avg       0.80      0.79      0.79       883\n",
      "   macro avg       0.61      0.60      0.60       883\n",
      "weighted avg       0.80      0.79      0.80       883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_3d.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041fe25-c1ca-40f3-937a-937697c62653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba696794-6adf-4e84-b89b-ddc4e16d8f2b",
   "metadata": {},
   "source": [
    "## To do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dad5c5-9d92-4898-a3fb-e276154131da",
   "metadata": {},
   "source": [
    "* Add detections to BEV - ground truth and predictions\n",
    "* Add visualizations along the way\n",
    "    * BEV - only in front of car --> bounds\n",
    "* Description of the model\n",
    "* RGB BEV encoding\n",
    "    * And how the compute_BEV function stores its data, plus WHY\n",
    "* Limitations of the model - no height info --> used average height by class\n",
    "* Add in average $z$ by class\n",
    "* Talk about how the model could be improved with sensor fusion\n",
    "* Different IoU thresholds in $2d$ vs $3d$ because overlap gets smaller as the number of dimensions increases...\n",
    "* Mention setting group slice whenever we want to access a particular group, and how we can't visualize in the app if the group slice is set to pcd\n",
    "* Mention right clicking and moving the mouse/mousepad to drag position in the $3d$ viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99987fdc-872f-4918-99ea-de3c43a1cbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f74dc-75cb-4c21-8345-75de2ceaf002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
