{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluate Detection Model with FiftyOne\n",
    "\n",
    "This walkthrough demonstrates how use FiftyOne to perform hands-on evaluation of your detection model.\n",
    "\n",
    "It covers the following concepts:\n",
    "* Loading a dataset with detections\n",
    "* Adding detection predictions\n",
    "* Sample-wise MSCOCO evaluation\n",
    "* Sorting and searching samples by model performance\n",
    "* Visualizing true-positives and false-positives\n",
    "* Querying your dataset for a custom insight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Install `torch` and `torchvision`, if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify as necessary (e.g., GPU install). See https://pytorch.org for options\n",
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the FiftyOne zoo and download the MSCOCO validation split to `~/fiftyone/coco-2017/validation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import torch, torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Media data is not copied when read into FiftyOne, when the Python process ends the dataset is deleted. Set `dataset.persistent=True` so that this process can be killed and the dataset can be loaded up again instantly in a new process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 'validation' already downloaded\n",
      "Loading 'coco-2017' split 'validation'\n",
      " 100%  5000/5000 [18.6s elapsed, 0s remai\n"
     ]
    }
   ],
   "source": [
    "dataset = foz.load_zoo_dataset(\"coco-2017\", \"validation\")\n",
    "dataset.persistent=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Faster-RCNN and download pretrained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on gpu if it is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions\n",
    "Run Faster-RCNN on every sample in the validation dataset and add detections to our FiftyOne dataset.\n",
    "Predictions are added to each sample in a new field we will call `faster_rcnn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0%     3/5000 [271.5ms elapsed, 7.5m r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erich/venvs/fo/lib/python3.6/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100%  5000/5000 [5.0m elapsed, 0s remain\n",
      "Finished adding predictions\n"
     ]
    }
   ],
   "source": [
    "# ETA is installed with FiftyOne\n",
    "# etai provides functionality to read images into memory\n",
    "import fiftyone.core.utils as fou\n",
    "import eta.core.image as etai\n",
    "import json\n",
    "from torchvision.transforms import functional as func\n",
    "\n",
    "labels_path = \"/home/erich/fiftyone/coco-2017/validation/labels.json\"\n",
    "with open(labels_path, \"r\") as labels_file:\n",
    "    classes = json.load(labels_file)[\"classes\"]\n",
    "\n",
    "# Add predictions\n",
    "with fou.ProgressBar() as pb:\n",
    "    for sample in pb(dataset):\n",
    "        image = etai.read(sample.filepath)\n",
    "        image = func.to_tensor(image).to(device)\n",
    "        c,h,w = image.shape\n",
    "\n",
    "        preds = model([image])[0]\n",
    "\n",
    "        labels = preds[\"labels\"].cpu().detach().numpy()\n",
    "        scores = preds[\"scores\"].cpu().detach().numpy()\n",
    "        boxes = preds[\"boxes\"].cpu().detach().numpy()\n",
    "\n",
    "        detections = []\n",
    "        for label, score, box in zip(labels, scores, boxes):\n",
    "            # Compute relative bounding box coordinates\n",
    "            x1, y1, x2, y2 = box\n",
    "            rel_box = [x1/w, y1/h, (x2-x1)/w, (y2-y1)/h]\n",
    "\n",
    "            detections.append(fo.Detection(\n",
    "                label=classes[label],\n",
    "                bounding_box=rel_box,\n",
    "                confidence=score\n",
    "            ))\n",
    "\n",
    "        sample[\"faster_rcnn\"] = fo.Detections(\n",
    "            detections=detections\n",
    "        )\n",
    "        sample.save()\n",
    "\n",
    "print(\"Finished adding predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Detections\n",
    "Use MSCOCO detection evaluation provided within FiftyOne to threshold detections and compute true and false positives for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FiftyOne allows you to write expressions to filter detections or classifications based on attributes of your field. For example, we can filter all of our Faster-RCNN detections to keep only boxes with a `confidence` higher than `0.75` and store them in a new `DatasetView`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "faster_rcnn_75 = dataset.filter_detections(\"faster_rcnn\", F(\"confidence\")>0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this filter more permanent we can clone the \"faster_rcnn\" field into \"faster_rcnn_75\" using the filtered view we just computed. This new field will now only contain bounding boxes with a `confidence > 0.75`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100%  5000/5000 [59.0s elapsed, 0s remai\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5000, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.clone_field(\"faster_rcnn\", \"faster_rcnn_75\", samples=faster_rcnn_75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match detections to ground truth and compute true and false positives according to MSCOCO evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections for each sample\n",
      " 100%  5000/5000 [1.6m elapsed, 0s remain\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.utils.cocoeval as fouc\n",
    "\n",
    "fouc.evaluate_detections(dataset, \"faster_rcnn_75\", \"ground_truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:           coco-2017-validation\n",
       "Persistent:     True\n",
       "Num samples:    5000\n",
       "Tags:           ['validation']\n",
       "Sample fields:\n",
       "    filepath:       fiftyone.core.fields.StringField\n",
       "    tags:           fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n",
       "    ground_truth:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    faster_rcnn:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    faster_rcnn_75: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    tp_iou_0_75:    fiftyone.core.fields.IntField\n",
       "    fp_iou_0_75:    fiftyone.core.fields.IntField\n",
       "    fn_iou_0_75:    fiftyone.core.fields.IntField"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every `Sample` now contains new fields `tp_iou_0_75`, `fp_iou_0_75`, and `fn_iou_0_75` corresponding to the total true positive, false positive, and false negative counts in your detections for an IoU of 0.75. This value can be changed using the `save_iou` kwarg in `evaluate_detections(dataset, \"faster_rcnn_75\", \"ground_truth\", save_iou=0.95)`\n",
    "\n",
    "Every `faster_rcnn_75` field in every `Sample` now contains a new `ground_truth_eval` field that contains `true_positives`, `false_positives`, and `false_negatives` ranging from IoUs `0_5`, `0_55`,..., to `0_95`.\n",
    "\n",
    "Every `Detection` in the `faster_rcnn_75` field now also has a `ground_truth_eval` field that contains:\n",
    "* The unique `eval_id` of that detection\n",
    "* The `ious` for every class of that detection with all ground truth detections of that class\n",
    "* The `matches` for 10 IoU values ranging from `0.5` to `0.95` that each contain the `gt_id` and `iou` of the ground truth detection that this predicted detection was matched with according to the pycocotools matching algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Detections\n",
    "Launch the FiftyOne app and easily view ground truth and predicted bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.dataset = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![launch](images/eval_dets/launch_app.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All fields are shown as togglable bubbles on the left sidebar which can be used to switch between ground truth detections, predictions, and thresholded predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bubbles](images/eval_dets/coco_gt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Views\n",
    "A `DatasetView` can also be used to search, sort, or slice your dataset for you to look at different views of the samples. \n",
    "\n",
    "Individual samples can be selected and a `DatasetView` can be created to look at just those samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_samples = session.selected\n",
    "session.view = dataset.select(selected_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![selected](images/eval_dets/selected.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the session dataset to show the entire dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.dataset = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tp_iou_0_75` was calculated for each sample during evaluation. We can make a `DatasetView` that sorts by `tp_iou_0_75` to look at the best predictions that the model had based on the number of true positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.view = dataset.sort_by(\"tp_iou_0_75\", reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tp_rev](images/eval_dets/tp_rev.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can use `fp_iou_0_75` to see the samples that our model performed the worst on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.view = dataset.sort_by(\"fp_iou_0_75\", reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fp_rev](images/eval_dets/fp_rev.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DatasetView` queries are extremely powerful. For example, if we just want to look at how our model performed on detecting small images, we can write a function that filters out any detections except ones where the box height multiplied by the box width is less than `0.005`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_boxes_view = dataset.filter_detections(\n",
    "    \"faster_rcnn_75\",\n",
    "    F(\"bounding_box\")[2] * F(\"bounding_box\")[3] < 0.005\n",
    ")\n",
    "\n",
    "session.view = small_boxes_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![small](images/eval_dets/small_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MSCOCO, bounding boxes can have an `iscrowd` attribute indicating that the box contains multiple instances of the same object. We can make a view of only samples with the `iscrowd` attribute on a detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "crowded_images_view = dataset.match(\n",
    "    F(\"ground_truth.detections\").filter(F(\"attributes.iscrowd.value\") == 1).length() > 0\n",
    ")\n",
    "\n",
    "session.view = crowded_images_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![crowd](images/eval_dets/crowded_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort that view of crowded images by false positive count in decreasing order to see samples that have a lot of false predictions and also include an `iscrowd` ground truth object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_crowded_images_view = crowded_images_view.sort_by(\n",
    "    \"fp_iou_0_75\", reverse=True\n",
    ")\n",
    "\n",
    "session.view = sorted_crowded_images_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![crowd_sort](images/eval_dets/crowded_sorted.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we compare this view to the one where we just sorted by false positives we can see something interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.view = dataset.sort_by(\"fp_iou_0_75\", reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fp_rev](images/eval_dets/fp_rev.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into specific examples, we can see that samples where there are a lot of false positives are ones where the underlying ground truth bounding box was missing the `iscrowd` attribute. This resulted in crowds of correct prediction to be labeled as false positive even though they are true positives. Knowing this, the MSCOCO labels could be refined to fix missing `iscrowd` attributes. \n",
    "\n",
    "This finding would have been nearly impossible to detect unless going through and looking at individual samples and searching by a variety of different criteria."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fo",
   "language": "python",
   "name": "fo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
