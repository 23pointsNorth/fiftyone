{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e55397-6322-4afa-9a01-bf1b75bb88d0",
   "metadata": {},
   "source": [
    "# Fine-tune YOLOv8 models for custom use cases with the help of FiftyOne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c4148b-c792-4165-b87d-c8120196fa02",
   "metadata": {},
   "source": [
    "Since its [initial release back in 2015](https://arxiv.org/abs/1506.02640), the You Only Look Once (YOLO) family of computer vision models has been one of the most popular in the field. In late 2022, [Ultralytics](https://github.com/ultralytics/ultralytics) announced the latest member of the YOLO family, [YOLOv8](https://docs.ultralytics.com/#ultralytics-yolov8), which comes with a new [backbone](https://arxiv.org/abs/2206.08016#:~:text=Many%20networks%20have%20been%20proposed,before%20and%20demonstrates%20its%20effectiveness.).\n",
    "\n",
    "The basic YOLOv8 detection and segmentation models, however, are general purpose, which means for custom use cases they may not be suitable out of the box. With FiftyOne, we can visualize and evaluate YOLOv8 model predictions, and better understand where the model's predictive power breaks down.\n",
    "\n",
    "In this walkthrough, we will show you how to load YOLOv8 model predictions into FiftyOne, and use insights from model evaluation to fine-tune a YOLOv8 model for your custom use case.\n",
    "\n",
    "Specifically, this walkthrough covers:\n",
    "\n",
    "* Loading YOLOv8 model predictions into FiftyOne\n",
    "* Evaluating YOLOv8 model predictions\n",
    "* Curating a dataset for fine-tuning\n",
    "* Fine-tuning YOLOv8 models\n",
    "* Comparing the performance of out-of-the-box and fine-tuned YOLOv8 models.\n",
    "\n",
    "**So, what's the takeaway?**\n",
    "\n",
    "FiftyOne can help you to achieve better performance using YOLOv8 models on real-time inference tasks for custom use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80e01a8-93c0-4e05-90f4-43bb40b3ce55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a017676-09ef-4034-b1b2-e41f7c5e6c02",
   "metadata": {},
   "source": [
    "To get started, you need to install [FiftyOne](https://docs.voxel51.com/getting_started/install.html) and [Ultralytics](https://github.com/ultralytics/ultralytics):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459573e-1cd7-4418-9b0f-1bf82f4c0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955e9f90-ef26-4a91-a119-06afb76f71c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784f2d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7f3799-7921-418b-a74a-dd6c006b5901",
   "metadata": {},
   "source": [
    "We will import the YOLO object from Ultralytics and use this to instantiate pretrained detection and segmentation models in Python. Along with the YOLOv8 architecture, Ultralytics released a set of pretrained models, with different sizes, for classification, detection, and segmentation tasks.\n",
    "\n",
    "For the purposes of illustration, we will use the smallest version, YOLOv8 Nano (YOLOv8n), but the same syntax will work for any of the pretrained models on the [Ultralytics YOLOv8 GitHub repo](https://github.com/ultralytics/ultralytics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ab27d-935c-41fb-b71a-081baa0a8551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "detection_model = YOLO(\"yolov8n.pt\")\n",
    "seg_model = YOLO(\"yolov8n-seg.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48ed457-60f1-4502-ac21-49f31ac51439",
   "metadata": {},
   "source": [
    "In Python, we can apply a YOLOv8 model to an individual image by passing the file path into the model call. For an image with file path `path/to/image.jpg`, running `detection_model(\"path/to/image.jpg\")` will generate a list containing a single `ultralytics.yolo.engine.results.Results` object. \n",
    "\n",
    "We can see this by applying the detection model to Ultralytics' test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb8fa56-804c-41e2-97b6-04857569e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = detection_model(\"https://ultralytics.com/images/bus.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41eca08-9342-45db-8426-86a309d585c9",
   "metadata": {},
   "source": [
    "A similar result can be obtained if we apply the segmentation model to an image. These results contain bounding boxes, class confidence scores, and integers representing class labels. For a complete discussion of these results objects, see the Ultralytics YOLOv8 [Results API Reference](https://docs.ultralytics.com/reference/results/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8da286b-7507-4fa7-b7a7-9fd4f00aa5cf",
   "metadata": {},
   "source": [
    "If we want to run tasks on all images in a directory, then we can do so from the command line with the YOLO Command Line Interface by specifying the task `[detect, segment, classify]` and mode `[train, val, predict, export]`, along with other arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8133ea3-35ab-43a6-93b6-32cabf5fe787",
   "metadata": {},
   "source": [
    "To run inference on a set of images, we must first put the data in the appropriate format. The best way to do so is to load your images into a FiftyOne `Dataset`, and then export the dataset in [YOLOv5Dataset](https://docs.voxel51.com/user_guide/dataset_creation/datasets.html#yolov5dataset) format, as YOLOv5 and YOLOv8 use the same data formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9526c81-5229-484b-a4c7-66621e9cec7f",
   "metadata": {},
   "source": [
    "## Load YOLOv8 predictions in FiftyOne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb32c2e-8f9d-4093-a656-ea571bc85009",
   "metadata": {},
   "source": [
    "In this walkthrough, we will look at YOLOv8’s predictions on a subset of the [MS COCO](https://cocodataset.org/#home) dataset. This is the dataset on which these models were trained, which means that they are likely to show close to peak performance on this data. Additionally, working with COCO data makes it easy for us to map model outputs to class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f777503d-203c-4aaa-be1e-fb67ac20360c",
   "metadata": {},
   "source": [
    "Load the images and ground truth object detections in COCO’s validation set from the [FiftyOne Dataset Zoo](https://docs.voxel51.com/user_guide/dataset_zoo/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb16e311-d189-44f2-9845-3224518136fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = foz.load_zoo_dataset(\n",
    "    'coco-2017',\n",
    "    split='validation',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3448e8-8406-4a67-ad6a-0a3cf4bdd4b4",
   "metadata": {},
   "source": [
    "We then generate a mapping from YOLO class predictions to COCO class labels. [COCO has 91 classes](https://cocodataset.org/#home), and YOLOv8, just like YOLOv3 and YOLOv5, ignores all of the numeric classes and [focuses on the remaining 80](https://imageai.readthedocs.io/en/latest/detection/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c929893-60a9-4be5-8308-1b87c57bdf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_classes = [c for c in dataset.default_classes if not c.isnumeric()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3d00e0-276c-4825-93ef-cdd621c7b612",
   "metadata": {},
   "source": [
    "Export the dataset into a directory `coco_val` in YOLO format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "717140d1-2705-442a-b608-3b3d6c5f278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_yolo_data(\n",
    "    samples, \n",
    "    export_dir, \n",
    "    classes, \n",
    "    label_field = \"ground_truth\", \n",
    "    split = None\n",
    "    ):\n",
    "\n",
    "    if type(split) == list:\n",
    "        splits = split\n",
    "        for split in splits:\n",
    "            export_yolo_data(\n",
    "                samples, \n",
    "                export_dir, \n",
    "                classes, \n",
    "                label_field, \n",
    "                split\n",
    "            )   \n",
    "    else:\n",
    "        if split is None:\n",
    "            split_view = samples\n",
    "            split = \"val\"\n",
    "        else:\n",
    "            split_view = samples.match_tags(split)\n",
    "\n",
    "        split_view.export(\n",
    "            export_dir=export_dir,\n",
    "            dataset_type=fo.types.YOLOv5Dataset,\n",
    "            label_field=label_field,\n",
    "            classes=classes,\n",
    "            split=split\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec8adf2-f689-4599-9c58-57501fbf40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_val_dir = \"coco_val\"\n",
    "export_yolo_data(dataset, coco_val_dir, coco_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bb1121-e2b9-43e5-aa82-bc5594193ff1",
   "metadata": {},
   "source": [
    "Then run inference on these images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcde98a-0afd-40bb-86f7-3fab2515c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict model=yolov8n.pt source=coco_val/images/val save_txt=True save_conf=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a720074-6ab3-4b72-8bc6-96163983428b",
   "metadata": {},
   "source": [
    "Running this inference generates a directory `runs/detect/predict/labels`, which will contain a separate `.txt` file for each image in the dataset, and a line for each object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b85b8f-c0f8-4866-ae87-cc44d41df1da",
   "metadata": {},
   "source": [
    "Each line is in the form: an integer for the class label, a class confidence score, and four values representing the bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a5ad723-f6a2-4fe7-9c29-cb656426496e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 0.663281 0.619718 0.0640625 0.201878 0.265856\n",
      "60 0.55625 0.619718 0.184375 0.225352 0.266771\n",
      "74 0.710938 0.307512 0.01875 0.0469484 0.277868\n",
      "60 0.860156 0.91784 0.279687 0.159624 0.278297\n",
      "72 0.744531 0.539906 0.101562 0.295775 0.356417\n",
      "75 0.888281 0.820423 0.0609375 0.241784 0.391675\n",
      "58 0.385156 0.457746 0.0640625 0.084507 0.420693\n",
      "56 0.609375 0.620892 0.090625 0.21831 0.50562\n",
      "56 0.650781 0.619718 0.0859375 0.215962 0.508265\n",
      "56 0.629687 0.619718 0.128125 0.220657 0.523211\n",
      "0 0.686719 0.535211 0.0828125 0.333333 0.712339\n",
      "56 0.505469 0.624413 0.0953125 0.230047 0.854189\n",
      "62 0.125 0.502347 0.23125 0.225352 0.927385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_file = \"runs/detect/predict/labels/000000000139.txt\"\n",
    "\n",
    "with open(label_file) as f: \n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2240717-18a2-4354-8c2b-c652dac4e8a4",
   "metadata": {},
   "source": [
    "We can read a YOLOv8 detection prediction file with $N$ detections into an $(N, 6)$ numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77a5e02b-e7a4-4c02-99af-07a387c9b624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yolo_detections_file(filepath):\n",
    "    detections = []\n",
    "    if not os.path.exists(filepath):\n",
    "        return np.array([])\n",
    "    \n",
    "    with open(filepath) as f:\n",
    "        lines = [line.rstrip('\\n').split(' ') for line in f]\n",
    "    \n",
    "    for line in lines:\n",
    "        detection = [float(l) for l in line]\n",
    "        detections.append(detection)\n",
    "    return np.array(detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd685bb-f7bf-4c28-bc02-1d9c4966f13e",
   "metadata": {},
   "source": [
    "From here, we need to convert these detections into FiftyOne’s [Detections](https://docs.voxel51.com/user_guide/using_datasets.html#object-detection) format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb96e27-c923-42dd-b5d8-46b6e6511972",
   "metadata": {},
   "source": [
    "YOLOv8 represents bounding boxes in a centered format with coordinates `[center_x, center_y, width, height]`, whereas [FiftyOne stores bounding boxes](https://docs.voxel51.com/user_guide/using_datasets.html#object-detection) in `[top-left-x, top-left-y, width, height]` format. We can make this conversion by \"un-centering\" the predicted bounding boxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aff712e4-10b2-470c-86d0-42cd324258e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _uncenter_boxes(boxes):\n",
    "    '''convert from center coords to corner coords'''\n",
    "    boxes[:, 0] -= boxes[:, 2]/2.\n",
    "    boxes[:, 1] -= boxes[:, 3]/2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992890d9-6ab7-45d7-a11b-04a51944bc15",
   "metadata": {},
   "source": [
    "Additionally, we can convert a list of class predictions (indices) to a list of class labels (strings) by passing in the class list:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3957e8af-dfd0-4e81-8ddb-4ef2ec191598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_class_labels(predicted_classes, class_list):\n",
    "    labels = (predicted_classes).astype(int)\n",
    "    labels = [class_list[l] for l in labels]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67cfeed-02c0-4264-9dbf-349c3049f2b8",
   "metadata": {},
   "source": [
    "Given the output of a `read_yolo_detections_file()` call, `yolo_detections`, we can generate the FiftyOne `Detections` object that captures this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24527175-44c3-42c7-9813-76fc2349b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_yolo_detections_to_fiftyone(\n",
    "    yolo_detections, \n",
    "    class_list\n",
    "    ):\n",
    "\n",
    "    detections = []\n",
    "    if yolo_detections.size == 0:\n",
    "        return fo.Detections(detections=detections)\n",
    "    \n",
    "    boxes = yolo_detections[:, 1:-1]\n",
    "    _uncenter_boxes(boxes)\n",
    "    \n",
    "    confs = yolo_detections[:, -1]\n",
    "    labels = _get_class_labels(yolo_detections[:, 0], class_list) \n",
    " \n",
    "    for label, conf, box in zip(labels, confs, boxes):\n",
    "        detections.append(\n",
    "            fo.Detection(\n",
    "                label=label,\n",
    "                bounding_box=box.tolist(),\n",
    "                confidence=conf\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return fo.Detections(detections=detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f0e43b-95de-4838-a840-2517017fb98b",
   "metadata": {},
   "source": [
    "The final ingredient is a function that takes in the file path of an image, and returns the file path of the corresponding YOLOv8 detection prediction text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665346d8-1b06-4a6b-8084-6339553a6c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_filepath(filepath, run_number = 1):\n",
    "    run_num_string = \"\"\n",
    "    if run_number != 1:\n",
    "        run_num_string = str(run_number)\n",
    "    filename = filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "    return f\"runs/detect/predict{run_num_string}/labels/{filename}.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073df59d-01a5-45a5-b458-c8aeaaaeab48",
   "metadata": {},
   "source": [
    "If you run multiple inference calls for the same task, the predictions results are stored in a directory with the next available integer appended to `predict` in the file path. You can account for this in the above function by passing in the `run_number` argument.\n",
    "\n",
    "Putting the pieces together, we can write a function that adds these YOLOv8 detections to all of the samples in our dataset efficiently by batching the read and write operations to the underlying [MongoDB database](https://docs.voxel51.com/environments/index.html#connecting-to-a-localhost-database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b23b8c8f-ac9d-40f4-8e0d-f0c2bd77f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_yolo_detections(\n",
    "    samples,\n",
    "    prediction_field,\n",
    "    prediction_filepath,\n",
    "    class_list\n",
    "    ):\n",
    "\n",
    "    prediction_filepaths = samples.values(prediction_filepath)\n",
    "    yolo_detections = [read_yolo_detections_file(pf) for pf in prediction_filepaths]\n",
    "    detections =  [convert_yolo_detections_to_fiftyone(yd, class_list) for yd in yolo_detections]\n",
    "    samples.set_values(prediction_field, detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7ced49-85ca-4280-969c-452a8eb7c8ef",
   "metadata": {},
   "source": [
    "Now we can rapidly add the detections in a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b661fea-d3fa-488d-8683-d62d90eefd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = dataset.values(\"filepath\")\n",
    "prediction_filepaths = [get_prediction_filepath(fp) for fp in filepaths]\n",
    "dataset.set_values(\n",
    "    \"yolov8n_det_filepath\", \n",
    "    prediction_filepaths\n",
    ")\n",
    "\n",
    "add_yolo_detections(\n",
    "    dataset, \n",
    "    \"yolov8n\", \n",
    "    \"yolov8n_det_filepath\", \n",
    "    coco_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb93d3-775b-4110-9b41-a2ca9c5b2d02",
   "metadata": {},
   "source": [
    "Now we can visualize these YOLOv8 model predictions on the samples in our dataset in the FiftyOne App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be5c69c-608e-41b6-9117-87d544ef1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27032b1f",
   "metadata": {},
   "source": [
    "![yolov8-base-predictions](images/yolov8_coco_val_predictions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a1304-01f4-4b6b-a776-ed45ed8380f8",
   "metadata": {},
   "source": [
    "**INSERT IMAGE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dbec94-c2f2-4379-a7fa-154fbd1f0a70",
   "metadata": {},
   "source": [
    "It is also worth noting that it is possible to convert YOLOv8 predictions directly from the output of a YOLO model call in Python, without first generating external prediction files and reading them in. Let’s see how this can be done for instance segmentations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77794bf3-85a4-46e5-bfc9-c55e841b1caf",
   "metadata": {},
   "source": [
    "Like detections, YOLOv8 stores instance segmentations with centered bounding boxes. In addition, [YOLOv8 stores a mask](https://docs.ultralytics.com/reference/results/#masks-api-reference) that covers the entire image, with only a rectangular region of that mask containing nonzero values. FiftyOne, on the other hand, [stores instance segmentations](https://docs.voxel51.com/user_guide/using_datasets.html#instance-segmentations) at `Detection` labels with a mask that only covers the given bounding box.\n",
    "\n",
    "We can convert from YOLOv8 instance segmentations to FiftyOne instance segmentations with this `convert_yolo_segmentations_to_fiftyone()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf0a3c12-236e-48a4-802d-19b85b807690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_yolo_segmentations_to_fiftyone(\n",
    "    yolo_segmentations, \n",
    "    class_list\n",
    "    ):\n",
    "\n",
    "    detections = []\n",
    "    boxes = yolo_segmentations.boxes.xywhn\n",
    "    if not boxes.shape or yolo_segmentations.masks is None:\n",
    "        return fo.Detections(detections=detections)\n",
    "    \n",
    "    _uncenter_boxes(boxes)\n",
    "    masks = yolo_segmentations.masks.masks\n",
    "    labels = _get_class_labels(yolo_segmentations.boxes.cls, class_list)\n",
    "\n",
    "    for label, box, mask in zip(labels, boxes, masks):\n",
    "        ## convert to absolute indices to index mask\n",
    "        w, h = mask.shape\n",
    "        tmp =  np.copy(box)\n",
    "        tmp[2] += tmp[0]\n",
    "        tmp[3] += tmp[1]\n",
    "        tmp[0] *= h\n",
    "        tmp[2] *= h\n",
    "        tmp[1] *= w\n",
    "        tmp[3] *= w\n",
    "        tmp = [int(b) for b in tmp]\n",
    "        y0, x0, y1, x1 = tmp\n",
    "        sub_mask = mask[x0:x1, y0:y1]\n",
    "       \n",
    "        detections.append(\n",
    "            fo.Detection(\n",
    "                label=label,\n",
    "                bounding_box = list(box),\n",
    "                mask = sub_mask.astype(bool)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return fo.Detections(detections=detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80547f08-2ead-44bc-9eb0-98209a4568cf",
   "metadata": {},
   "source": [
    "Looping through all samples in the dataset, we can add the predictions from our `seg_model`, and then view these predicted masks in the FiftyOne App."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ddd44e-2b8b-41ca-94f6-1a28e1e102fa",
   "metadata": {},
   "source": [
    "## Evaluate YOLOv8 model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c4e37-9086-4bac-bff9-3f51a85a0492",
   "metadata": {},
   "source": [
    "Now that we have YOLOv8 predictions loaded onto the images in our dataset, we can evaluate the quality of these predictions using FiftyOne’s [Evaluation API](https://docs.voxel51.com/user_guide/evaluation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d78170-220c-4015-a667-55c2beb4defc",
   "metadata": {},
   "source": [
    "To evaluate the object detections in the `yolov8_det` field relative to the `ground_truth` detections field, we can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600051bb-18c1-4118-ba00-419a22bbaddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_results = dataset.evaluate_detections(\n",
    "    \"yolov8n\", \n",
    "    eval_key=\"eval\",\n",
    "    compute_mAP=True,\n",
    "    gt_field=\"ground_truth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980c8901-5407-454b-a2eb-ba7c03d08f57",
   "metadata": {},
   "source": [
    "We can then get the [mean average precision](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173) (mAP) of the model’s predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91fee641-f588-4d9f-9177-7b84e1a7823b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP = 0.3121319189417518\n"
     ]
    }
   ],
   "source": [
    "mAP = detection_results.mAP()\n",
    "print(f\"mAP = {mAP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f093f2c-dc78-4096-8cae-46d740554f12",
   "metadata": {},
   "source": [
    "We can also look at the model’s performance on the 20 most common object classes in the dataset, where it has seen the most examples so the statistics are most meaningful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf2f85cc-c2c6-4a97-ba80-6441612e9fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "       person       0.85      0.68      0.76     11573\n",
      "          car       0.71      0.52      0.60      1971\n",
      "        chair       0.62      0.34      0.44      1806\n",
      "         book       0.61      0.12      0.20      1182\n",
      "       bottle       0.68      0.39      0.50      1051\n",
      "          cup       0.61      0.44      0.51       907\n",
      " dining table       0.54      0.42      0.47       697\n",
      "traffic light       0.66      0.36      0.46       638\n",
      "         bowl       0.63      0.49      0.55       636\n",
      "      handbag       0.48      0.12      0.19       540\n",
      "         bird       0.79      0.39      0.52       451\n",
      "         boat       0.58      0.29      0.39       430\n",
      "        truck       0.57      0.35      0.44       415\n",
      "        bench       0.58      0.27      0.37       413\n",
      "     umbrella       0.65      0.52      0.58       423\n",
      "          cow       0.81      0.61      0.70       397\n",
      "       banana       0.68      0.34      0.45       397\n",
      "       carrot       0.56      0.29      0.38       384\n",
      "   motorcycle       0.77      0.58      0.66       379\n",
      "     backpack       0.51      0.16      0.24       371\n",
      "\n",
      "    micro avg       0.76      0.52      0.61     25061\n",
      "    macro avg       0.64      0.38      0.47     25061\n",
      " weighted avg       0.74      0.52      0.60     25061\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts = dataset.count_values(\"ground_truth.detections.label\")\n",
    "\n",
    "top20_classes = sorted(\n",
    "    counts, \n",
    "    key=counts.get, \n",
    "    reverse=True\n",
    ")[:20]\n",
    "\n",
    "detection_results.print_report(classes=top20_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8c9ce-aba1-46f1-b9bb-3c872bd06ae9",
   "metadata": {},
   "source": [
    "From the output of `print_report()`, we can see that this model performs decently well, but certainly has its limitations. While its precision is relatively good on average, it is lacking when it comes to recall. This is especially pronounced for certain classes like the `book` class.\n",
    "\n",
    "Fortunately, we can dig deeper into these results with FiftyOne. Using the FiftyOne App, we can for instance filter by class for both ground truth and predicted detections so that only `book` detections appear in the samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2963e05-b9ff-40ea-a751-2057bc540edb",
   "metadata": {},
   "source": [
    "![yolov8-book-predictions](images/yolov8_coco_val_books_modal.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35113a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c6aec1-c1f6-427c-82e1-1cd5d8d50c78",
   "metadata": {},
   "source": [
    "Scrolling through the samples in the sample grid, we can see that a lot of the time, COCO’s purported *ground truth* labels for the `book` class appear to be imperfect. Sometimes, individual books are bounded, other times rows or whole bookshelves are encompassed in a single box, and yet other times books are entirely unlabeled. Unless our desired computer vision application specifically requires good `book` detection, this should probably not be a point of concern when we are assessing the quality of the model. After all, the quality of a model is limited by the quality of the data it is trained on - this is why data-centric approaches to computer vision are so important!\n",
    "\n",
    "For other classes like the `bird` class, however, there appear to be challenges. One way to see this is to filter for `bird` ground truth detections and then convert to an [EvaluationPatchesView](https://docs.voxel51.com/api/fiftyone.core.patches.html#fiftyone.core.patches.EvaluationPatchesView). Some of these recall errors appear to be related to small features, where the resolution is poor.\n",
    "\n",
    "In other cases though, quick inspection confirms that the object is clearly a bird. This means that there is likely room for improvement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02067175-04c4-4855-a58e-17389ca6a381",
   "metadata": {},
   "source": [
    "![yolov8-base-bird_patches](images/yolov8_coco_val_bird_patch_view.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66d63a-b831-451a-8460-be4dd30cc1ee",
   "metadata": {},
   "source": [
    "## Curate data for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ccf433-5ceb-43d5-8a52-2ce06b9588f9",
   "metadata": {},
   "source": [
    "For the remainder of this walkthrough, we will pretend that we are working for a bird conservancy group, putting computer vision models in the field to track and protect endangered species. Our goal is to fine-tune a YOLOv8 detection model to detect birds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c060f7-628f-4630-80e2-d9e0c940f4a0",
   "metadata": {},
   "source": [
    "We will use the COCO validation dataset above as our test set. Since we are only concerned with detecting birds, we can filter out all non-`bird` ground truth detections using `filter_labels()`. We will also filter out the non- `bird` predictions, but will pass the `only_matches = False` argument into `filter_labels()` to make sure we keep images that have ground truth `bird` detections without YOLOv8n `bird` predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33281ed-f75e-4ad8-ab11-385fee9009b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset.filter_labels(\n",
    "    \"ground_truth\", \n",
    "    F(\"label\") == \"bird\"\n",
    ").filter_labels(\n",
    "    \"yolov8n\", \n",
    "    F(\"label\") == \"bird\",\n",
    "    only_matches=False\n",
    ").clone()\n",
    "\n",
    "test_dataset.name = \"birds-test-dataset\"\n",
    "test_dataset.persistent = True\n",
    "\n",
    "## set classes to just include birds\n",
    "classes = [\"bird\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1da5a78-50b9-4a5f-a192-73398516dcb2",
   "metadata": {},
   "source": [
    "We then give the dataset a name, make it persistent, and save it to the underlying database. This test set has only 125 images, which we can visualize in the FiftyOne App."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5a9af8-fe89-4f30-ac33-43583c41fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f7ed1d6-26bf-48d0-9e62-dc61c81763fc",
   "metadata": {},
   "source": [
    "![yolov8-birds-test-view](images/yolov8_bird_test_view.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a3c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb854711-4467-497f-9883-acafbde5483b",
   "metadata": {},
   "source": [
    "We can also run `evaluate_detections()` on this data to evaluate the YOLOv8n model's performance on images with ground truth bird detections. We will store the results under the `base` evaluation key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64923c16-dae6-408b-bc69-0434e669ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_bird_results = test_dataset.evaluate_detections(\n",
    "    \"yolov8n\", \n",
    "    eval_key=\"base\",\n",
    "    compute_mAP=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43b4781f-8abe-4997-9649-ae2bf8c04d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base mAP = 0.24897924786479841\n"
     ]
    }
   ],
   "source": [
    "mAP = base_bird_results.mAP()\n",
    "print(f\"Base mAP = {mAP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b0a0ad1-2653-4699-90f8-82a0f509edf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bird       0.87      0.39      0.54       451\n",
      "\n",
      "   micro avg       0.87      0.39      0.54       451\n",
      "   macro avg       0.87      0.39      0.54       451\n",
      "weighted avg       0.87      0.39      0.54       451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_bird_results.print_report(classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699a0bc2-80dc-46f4-9295-1f72a9eede39",
   "metadata": {},
   "source": [
    "We note that while the recall is the same as in the initial evaluation report over the entire COCO validation split, the precision is higher. This means there are images that have YOLOv8n `bird` predictions but not ground truth `bird` detections.\n",
    "\n",
    "The final step in preparing this test set is exporting the data into YOLOv8 format so we can run inference on just these samples with our fine-tuned model when we are done training. We will do so using the `export_yolo_data()` function we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf275148-8344-4a1b-be6c-4c3ae28d1701",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_yolo_data(\n",
    "    test_dataset, \n",
    "    \"birds_test\", \n",
    "    classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f710e1-3d00-45d4-96a6-ccc7dee4e138",
   "metadata": {},
   "source": [
    "Now we choose the data on which we will fine-tune the base YOLOv8 model. Our goal is to generate a high-quality training dataset whose examples cover all expected scenarios in that subset. \n",
    "\n",
    "In general, this is both an art and a science, and it can involve a variety of techniques, including \n",
    "\n",
    "* pulling in data from other datasets\n",
    "* annotating more data that you’ve already collected with ground truth labels, \n",
    "* augmenting your data with tools like [Albumentations](https://albumentations.ai/)\n",
    "* generating synthetic data with [diffusion models](https://blog.roboflow.com/synthetic-data-with-stable-diffusion-a-guide/) or [GANs](https://towardsai.net/p/l/gans-for-synthetic-data-generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac58e1-d763-4e03-a8da-bbd95ad5dc4e",
   "metadata": {},
   "source": [
    "We’ll take the first approach and incorporate existing high-quality data from Google’s [Open Images dataset](https://storage.googleapis.com/openimages/web/index.html). For a thorough tutorial on how to work with Open Images data, see [Loading Open Images V6 and custom datasets with FiftyOne](https://medium.com/voxel51/loading-open-images-v6-and-custom-datasets-with-fiftyone-18b5334851c3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551906a4-f586-4639-b4fe-b4d7c3812313",
   "metadata": {},
   "source": [
    "The COCO training data on which YOLOv8 was trained contains $3,237$ images with `bird` detections. Open Images is more expansive, with the train, test, and validation splits together housing $20k+$ images with `Bird` detections.\n",
    "\n",
    "Let’s create our training dataset. First, we’ll create a dataset, `train_dataset`, by loading the `bird` detection labels from the COCO train split using the [FiftyOne Dataset Zoo](https://docs.voxel51.com/user_guide/dataset_zoo/datasets.html), and cloning this into a new `Dataset` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a75282-224e-41a0-b53d-e5c259bdc0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = foz.load_zoo_dataset(\n",
    "    'coco-2017',\n",
    "    split='train',\n",
    "    classes=classes\n",
    ").clone()\n",
    "\n",
    "train_dataset.name = \"birds-train-data\"\n",
    "train_dataset.persistent = True\n",
    "train_dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dfc870-3cfe-46df-9e27-95b2e63d735a",
   "metadata": {},
   "source": [
    "Then, we’ll load Open Images samples with `Bird` detection labels, passing in `only_matching=True` to only load the `Bird` labels. We then map these labels into COCO label format by changing `Bird` into `bird`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9e94f-8073-42b3-8d4f-bac43609fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "oi_samples = foz.load_zoo_dataset(\n",
    "    \"open-images-v6\",\n",
    "    classes = [\"Bird\"],\n",
    "    only_matching=True,\n",
    "    label_types=\"detections\"\n",
    ").map_labels(\n",
    "    \"ground_truth\",\n",
    "    {\"Bird\":\"bird\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b08e8f-56d1-4205-abba-3096643abc39",
   "metadata": {},
   "source": [
    "We can add these new samples into our training dataset with `merge_samples()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4871287-5f5f-40fb-bc28-ffa34391c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.merge_samples(oi_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fa2773-2699-44d7-a7f9-26ac02281ff7",
   "metadata": {},
   "source": [
    "This dataset contains $24,226$ samples with `bird` labels, or more than seven times as many birds as the base YOLOv8n model was trained on. In the next section, we'll demonstrate how to fine-tune the model on this data using the [YOLO Trainer class](https://docs.ultralytics.com/reference/base_trainer/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73d8cd2-b8dd-49f6-8a28-235d22a76401",
   "metadata": {},
   "source": [
    "## Fine-tune a YOLOv8 detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913370fe-ee12-4d89-a8d9-9f6c10d7a3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a925e6f7-16a2-4c0d-9bd2-2dd8d51fabc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "885d5529-2e1f-4a3c-98e9-db3de828ac70",
   "metadata": {},
   "source": [
    "      Image sizes 640 train, 640 val\n",
    "\n",
    "      Using 8 dataloader workers\n",
    "\n",
    "      Logging results to runs/detect/train\n",
    "  \n",
    "      Starting training for 60 epochs...\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "      1/60       6.65G      1.392      1.627      1.345         22        640: 1\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
    "                   all       4845      12487      0.677      0.524      0.581      0.339\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "      2/60       9.58G      1.446      1.407      1.395         30        640: 1\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
    "                   all       4845      12487      0.669       0.47       0.54      0.316\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "      3/60       9.58G       1.54      1.493      1.462         29        640: 1\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
    "                   all       4845      12487      0.529      0.329      0.349      0.188\n",
    "\n",
    "                                            ......\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "     58/60       9.59G      1.263     0.9489      1.277         47        640: 1\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
    "                   all       4845      12487      0.751      0.631      0.708      0.446\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "     59/60       9.59G      1.264     0.9476      1.277         29        640: 1\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
    "                   all       4845      12487      0.752      0.631      0.708      0.446\n",
    "\n",
    "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
    "     60/60       9.59G      1.257     0.9456      1.274         41        640: 1\n",
    "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
    "                   all       4845      12487      0.752      0.631      0.709      0.446"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2dd8096-1f3e-47fd-b412-0dabcf38123b",
   "metadata": {},
   "source": [
    "![yolov8-finetune-predictions](images/yolov8_finetune_predictions_app.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c04dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.freeze()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2f80940-4a4c-4f91-a42d-a96fb39c5445",
   "metadata": {},
   "source": [
    "![yolov8-finetune_fn](images/yolov8_finetune_fn_predictions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0328cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.freeze()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63983d59-fa15-42e8-a68a-8354cd13fb59",
   "metadata": {},
   "source": [
    "![yolov8-finetune-fp](images/yolov8_finetune_fp_predictions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e180d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "92cc4cd4-1815-41f8-ad2d-94b944d1e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_bird_results = test_dataset.load_evaluation_results(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8b0d246-c73d-4a6b-ac4b-855c2dacd4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_results = dataset.load_evaluation_results('eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77527d28-d27b-4d16-8613-14d623ff0d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e4454d7-6bc6-4a8c-bf38-77228c6b11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = fo.load_dataset(\"coco_val_eval\")\n",
    "test_dataset = fo.load_dataset('birds-test-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d44508b-197b-4844-bab4-134ff535ddbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['birds-test-dataset',\n",
       " 'birds-train-data',\n",
       " 'coco-validation-seg',\n",
       " 'coco_val_eval']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fo.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5deb8d6f-79c9-49e4-86c4-47c363e30eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    }
   ],
   "source": [
    "session = fo.launch_app(dataset, auto=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "330b2ede-cb91-44d7-abbc-22c563fc172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.view = dataset.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2bfbe3-cfdf-4e07-ae51-5a0b0ceca895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f00dd-b2a5-4669-8d11-38e66af11161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f66d9cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.config.dataset_zoo_dir = '/scratch/user/jacob/zoo_datasets/'\n",
    "fo.config.default_dataset_dir = '/scratch/user/jacob/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d0ed22e-87d7-42fd-bf0f-878790e68c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023.02.08.20.17.46',\n",
       " 'birds-test-dataset',\n",
       " 'birds-train-data',\n",
       " 'coco-validation-seg',\n",
       " 'coco_val_eval']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fo.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e09cc709-765e-4067-9148-33cdb2a340a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fo.load_dataset('coco_val_eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd1d656-b383-4881-a6f3-8bad1ebdd03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b69a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.add_sample_field(\"precision\", fo.FloatField)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27099f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_vals = np.array(dataset.values(\"eval_tp\"))\n",
    "fp_vals = np.array(dataset.values(\"eval_fp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fad81858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28189/2879195052.py:1: RuntimeWarning: invalid value encountered in divide\n",
      "  precision_vals = tp_vals/(tp_vals+fp_vals)\n"
     ]
    }
   ],
   "source": [
    "precision_vals = tp_vals/(tp_vals+fp_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7251f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_view = dataset.match(F(\"eval_tp\") > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41b75420",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_vals = np.array(non_empty_view.values(\"eval_tp\"))\n",
    "fp_vals = np.array(non_empty_view.values(\"eval_fp\"))\n",
    "precision_vals = tp_vals/(tp_vals+fp_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aeccd47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_view.set_values(\"precision\", precision_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c909e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_empty_view.set_field(\"precision\", F(\"eval_tp\")/(F(\"eval_tp\") + F(\"eval_fp\")), _allow_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9017f4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_precision_view = non_empty_view.sort_by(\"precision\", reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "513855a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_precision_view.values(\"precision\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9dac06cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_view(\"high_precision\",high_precision_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3d35678",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_precision_view = non_empty_view.sort_by(\"precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91d9f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_view(\"low_precision\",low_precision_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d195e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061626a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6374e883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355a101c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e670062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mAP_vals = []\n",
    "# sample_ids = dataset.values('id')[1850:]\n",
    "# for sid in sample_ids:\n",
    "#     view = dataset[[sid]]\n",
    "#     view_res = view.evaluate_detections(\n",
    "#         \"yolov8n\", \n",
    "#         eval_key=\"single\",\n",
    "#         compute_mAP=True,\n",
    "#     );\n",
    "#     mAP_vals.append(view_res.mAP())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e192f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mAP_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2900bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6f41211",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = dataset[[dataset.first().id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ba1fe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |█████████████████████| 1/1 [53.8ms elapsed, 0s remaining, 18.6 samples/s] \n",
      "Performing IoU sweep...\n",
      " 100% |█████████████████████| 1/1 [28.1ms elapsed, 0s remaining, 35.5 samples/s] \n"
     ]
    }
   ],
   "source": [
    "view_res = view.evaluate_detections(\n",
    "    \"yolov8n\", \n",
    "    eval_key=\"single\",\n",
    "    compute_mAP=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6de0ce87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21678217821782178"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_res.mAP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8bd00",
   "metadata": {},
   "source": [
    "## Base Model on COCO Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba644605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to '/scratch/user/jacob/zoo_datasets/coco-2017/validation' if necessary\n",
      "Found annotations at '/scratch/user/jacob/zoo_datasets/coco-2017/raw/instances_val2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading 'coco-2017' split 'validation'\n",
      " 100% |███████████████| 5000/5000 [20.1s elapsed, 0s remaining, 246.8 samples/s]      \n",
      "Dataset 'coco-2017-validation' created\n"
     ]
    }
   ],
   "source": [
    "dataset = foz.load_zoo_dataset(\n",
    "    'coco-2017',\n",
    "    split='validation',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3115c155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sample: {\n",
       "    'id': '63e3770d011bb4c2e294fe10',\n",
       "    'media_type': 'image',\n",
       "    'filepath': '/scratch/user/jacob/zoo_datasets/coco-2017/validation/data/000000000139.jpg',\n",
       "    'tags': ['validation'],\n",
       "    'metadata': <ImageMetadata: {\n",
       "        'size_bytes': None,\n",
       "        'mime_type': None,\n",
       "        'width': 640,\n",
       "        'height': 426,\n",
       "        'num_channels': None,\n",
       "    }>,\n",
       "    'ground_truth': <Detections: {\n",
       "        'detections': [\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fdfc',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'potted plant',\n",
       "                'bounding_box': [\n",
       "                    0.37028125,\n",
       "                    0.3345305164319249,\n",
       "                    0.038593749999999996,\n",
       "                    0.16314553990610328,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'furniture',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fdfd',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'tv',\n",
       "                'bounding_box': [\n",
       "                    0.010984375000000001,\n",
       "                    0.39380281690140845,\n",
       "                    0.23331249999999998,\n",
       "                    0.22269953051643193,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'electronic',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fdfe',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'tv',\n",
       "                'bounding_box': [\n",
       "                    0.8706406250000001,\n",
       "                    0.491056338028169,\n",
       "                    0.127109375,\n",
       "                    0.18481220657276995,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'electronic',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fdff',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'chair',\n",
       "                'bounding_box': [\n",
       "                    0.56090625,\n",
       "                    0.5118544600938968,\n",
       "                    0.0875,\n",
       "                    0.24138497652582158,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'furniture',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe00',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'chair',\n",
       "                'bounding_box': [\n",
       "                    0.454203125,\n",
       "                    0.5117370892018779,\n",
       "                    0.096609375,\n",
       "                    0.2311737089201878,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'furniture',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe01',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'chair',\n",
       "                'bounding_box': [\n",
       "                    0.645625,\n",
       "                    0.5234976525821596,\n",
       "                    0.047140625000000005,\n",
       "                    0.19098591549295774,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'furniture',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe02',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'chair',\n",
       "                'bounding_box': [\n",
       "                    0.4959375,\n",
       "                    0.5146478873239437,\n",
       "                    0.03371875,\n",
       "                    0.02720657276995305,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'furniture',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe03',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'person',\n",
       "                'bounding_box': [\n",
       "                    0.645,\n",
       "                    0.3699765258215963,\n",
       "                    0.082890625,\n",
       "                    0.3239671361502347,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'person',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe04',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'person',\n",
       "                'bounding_box': [\n",
       "                    0.600671875,\n",
       "                    0.40424882629107983,\n",
       "                    0.023625,\n",
       "                    0.08389671361502347,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'person',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe05',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'microwave',\n",
       "                'bounding_box': [\n",
       "                    0.80034375,\n",
       "                    0.482981220657277,\n",
       "                    0.02303125,\n",
       "                    0.037488262910798126,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'appliance',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe06',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'refrigerator',\n",
       "                'bounding_box': [\n",
       "                    0.77046875,\n",
       "                    0.40924882629107984,\n",
       "                    0.031703125,\n",
       "                    0.2542488262910798,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'appliance',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe07',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'book',\n",
       "                'bounding_box': [\n",
       "                    0.944953125,\n",
       "                    0.7180516431924883,\n",
       "                    0.02240625,\n",
       "                    0.10730046948356808,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'indoor',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe08',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'book',\n",
       "                'bounding_box': [\n",
       "                    0.9581875,\n",
       "                    0.7235680751173709,\n",
       "                    0.020125,\n",
       "                    0.10901408450704225,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'indoor',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe09',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'clock',\n",
       "                'bounding_box': [\n",
       "                    0.699640625,\n",
       "                    0.2843192488262911,\n",
       "                    0.021828125,\n",
       "                    0.05136150234741784,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'indoor',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe0a',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'vase',\n",
       "                'bounding_box': [\n",
       "                    0.8579062499999999,\n",
       "                    0.7263615023474178,\n",
       "                    0.0573125,\n",
       "                    0.2104929577464789,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'indoor',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe0b',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'vase',\n",
       "                'bounding_box': [\n",
       "                    0.5480625,\n",
       "                    0.4902347417840376,\n",
       "                    0.017765625,\n",
       "                    0.05293427230046949,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'indoor',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe0c',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'chair',\n",
       "                'bounding_box': [\n",
       "                    0.644140625,\n",
       "                    0.514131455399061,\n",
       "                    0.015046875000000001,\n",
       "                    0.029389671361502348,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'furniture',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe0d',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'vase',\n",
       "                'bounding_box': [\n",
       "                    0.37693750000000004,\n",
       "                    0.4577230046948357,\n",
       "                    0.022218750000000002,\n",
       "                    0.04138497652582159,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'indoor',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe0e',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'vase',\n",
       "                'bounding_box': [\n",
       "                    0.526234375,\n",
       "                    0.46830985915492956,\n",
       "                    0.015203125000000001,\n",
       "                    0.039272300469483566,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'indoor',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe0f',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'dining table',\n",
       "                'bounding_box': [\n",
       "                    0.5018906249999999,\n",
       "                    0.5427699530516432,\n",
       "                    0.19618750000000001,\n",
       "                    0.20875586854460096,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'furniture',\n",
       "                'iscrowd': 0,\n",
       "            }>,\n",
       "        ],\n",
       "    }>,\n",
       "}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "275ed95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [c for c in dataset.default_classes if not c.isnumeric()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07213150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_yolo_data(samples, export_dir, classes, label_field = \"ground_truth\", split = None):\n",
    "    if type(split) == list:\n",
    "        splits = split\n",
    "        for split in splits:\n",
    "            export_yolo_data(samples, export_dir, classes, label_field, split)   \n",
    "    else:\n",
    "        if split is None:\n",
    "            split_view = samples\n",
    "            split = \"val\"\n",
    "        else:\n",
    "            split_view = samples.match_tags(split)\n",
    "\n",
    "        split_view.export(\n",
    "            export_dir=export_dir,\n",
    "            dataset_type=fo.types.YOLOv5Dataset,\n",
    "            label_field=label_field,\n",
    "            classes=classes,\n",
    "            split=split\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8b31639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mbirds_test\u001b[0m/  \u001b[01;34mcoco_val\u001b[0m/  \u001b[01;34monly_birds\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /scratch/user/jacob/yolov8/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42a9115b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 5000/5000 [10.6s elapsed, 0s remaining, 522.8 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "export_yolo_data(dataset, \"/scratch/user/jacob/yolov8/coco_val\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f05232e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.30 🚀 Python-3.9.13 torch-1.13.1 CUDA:0 (NVIDIA TITAN V, 12067MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
      "4907 labels saved to runs/detect/predict/labels\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=predict model=yolov8n.pt source=/scratch/user/jacob/yolov8/coco_val/images/val save_txt=True save_conf=True verbose=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b81ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _uncenter_boxes(boxes):\n",
    "    '''convert from center coords to corner coords'''\n",
    "    boxes[:, 0] -= boxes[:, 2]/2.\n",
    "    boxes[:, 1] -= boxes[:, 3]/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d02979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_class_labels(predicted_classes, class_list):\n",
    "    labels = (predicted_classes).astype(int)\n",
    "    labels = [class_list[l] for l in labels]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a6c5cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_yolo_detections_to_fiftyone(yolo_detections, class_list):\n",
    "    detections = []\n",
    "    if yolo_detections.size == 0:\n",
    "        return fo.Detections(detections=detections)\n",
    "    \n",
    "    boxes = yolo_detections[:, 1:-1]\n",
    "    _uncenter_boxes(boxes)\n",
    "    \n",
    "    confs = yolo_detections[:, -1]\n",
    "    labels = _get_class_labels(yolo_detections[:, 0], class_list)  \n",
    "\n",
    "    for label, conf, box in zip(labels, confs, boxes):\n",
    "        detections.append(\n",
    "            fo.Detection(\n",
    "                label=label,\n",
    "                bounding_box=box.tolist(),\n",
    "                confidence=conf\n",
    "            )\n",
    "        )\n",
    "    return fo.Detections(detections=detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32dd488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_filepath(filepath, run_number = 1):\n",
    "    run_num_string = \"\"\n",
    "    if run_number != 1:\n",
    "        run_num_string = str(run_number)\n",
    "    filename = filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "    return \"runs/detect/predict{}/labels/\".format(run_num_string) + filename + \".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "731bede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yolo_detections_file(filepath):\n",
    "    detections = []\n",
    "    if not os.path.exists(filepath):\n",
    "        return np.array([])\n",
    "    \n",
    "    with open(filepath) as f:\n",
    "        lines = [line.rstrip('\\n').split(' ') for line in f]\n",
    "    \n",
    "    for line in lines:\n",
    "        detection = [float(l) for l in line]\n",
    "        detections.append(detection)\n",
    "    return np.array(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16e6db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_yolo_detections(\n",
    "    samples,\n",
    "    prediction_field,\n",
    "    prediction_filepath,\n",
    "    class_list\n",
    "):\n",
    "    prediction_filepaths = samples.values(prediction_filepath)\n",
    "    yolo_detections = [read_yolo_detections_file(pf) for pf in prediction_filepaths]\n",
    "    detections =  [convert_yolo_detections_to_fiftyone(yd, class_list) for yd in yolo_detections]\n",
    "    samples.set_values(prediction_field, detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daca99f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = dataset.values(\"filepath\")\n",
    "prediction_filepaths = [get_prediction_filepath(fp) for fp in filepaths]\n",
    "dataset.set_values(\"yolov8n_det_filepath\", prediction_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59bb9736",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_yolo_detections(\n",
    "    dataset, \n",
    "    \"yolov8n\", \n",
    "    \"yolov8n_det_filepath\", \n",
    "    classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b81c9a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |███████████████| 5000/5000 [1.1m elapsed, 0s remaining, 84.2 samples/s]       \n",
      "Performing IoU sweep...\n",
      " 100% |███████████████| 5000/5000 [49.3s elapsed, 0s remaining, 107.4 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "detection_results = dataset.evaluate_detections(\n",
    "    \"yolov8n\", \n",
    "    eval_key=\"eval\",\n",
    "    compute_mAP=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13c0f77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP = 0.3121319189417518\n"
     ]
    }
   ],
   "source": [
    "# detection_results.mAP()\n",
    "print(\"mAP = {}\".format(detection_results.mAP()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bb565aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_view = dataset.sort_by(\"eval_fn\", reverse=True)\n",
    "dataset.save_view(\"fn_view\", fn_view)\n",
    "# session = fo.launch_app(fn_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd620e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471698b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ecac28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sample: {\n",
       "    'id': '63e3770d011bb4c2e294fe10',\n",
       "    'media_type': 'image',\n",
       "    'filepath': '/scratch/user/jacob/zoo_datasets/coco-2017/validation/data/000000000139.jpg',\n",
       "    'tags': ['validation'],\n",
       "    'metadata': <ImageMetadata: {\n",
       "        'size_bytes': None,\n",
       "        'mime_type': None,\n",
       "        'width': 640,\n",
       "        'height': 426,\n",
       "        'num_channels': None,\n",
       "    }>,\n",
       "    'ground_truth': <Detections: {\n",
       "        'detections': [\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fdfc',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'potted plant',\n",
       "                'bounding_box': [\n",
       "                    0.37028125,\n",
       "                    0.3345305164319249,\n",
       "                    0.038593749999999996,\n",
       "                    0.16314553990610328,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'furniture',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'fn',\n",
       "                'eval_id': '',\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fdfd',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'tv',\n",
       "                'bounding_box': [\n",
       "                    0.010984375000000001,\n",
       "                    0.39380281690140845,\n",
       "                    0.23331249999999998,\n",
       "                    0.22269953051643193,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'electronic',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e37734011bb4c2e295a13d',\n",
       "                'eval_iou': 0.9536521929850259,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fdfe',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'tv',\n",
       "                'bounding_box': [\n",
       "                    0.8706406250000001,\n",
       "                    0.491056338028169,\n",
       "                    0.127109375,\n",
       "                    0.18481220657276995,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'electronic',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'fn',\n",
       "                'eval_id': '',\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fdff',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'chair',\n",
       "                'bounding_box': [\n",
       "                    0.56090625,\n",
       "                    0.5118544600938968,\n",
       "                    0.0875,\n",
       "                    0.24138497652582158,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'furniture',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e37734011bb4c2e295a13a',\n",
       "                'eval_iou': 0.576508295303288,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe00',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'chair',\n",
       "                'bounding_box': [\n",
       "                    0.454203125,\n",
       "                    0.5117370892018779,\n",
       "                    0.096609375,\n",
       "                    0.2311737089201878,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'furniture',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e37734011bb4c2e295a13c',\n",
       "                'eval_iou': 0.9173482595434582,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe01',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'chair',\n",
       "                'bounding_box': [\n",
       "                    0.645625,\n",
       "                    0.5234976525821596,\n",
       "                    0.047140625000000005,\n",
       "                    0.19098591549295774,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'furniture',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e37734011bb4c2e295a131',\n",
       "                'eval_iou': 0.6961515600731741,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe02',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'chair',\n",
       "                'bounding_box': [\n",
       "                    0.4959375,\n",
       "                    0.5146478873239437,\n",
       "                    0.03371875,\n",
       "                    0.02720657276995305,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'furniture',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'fn',\n",
       "                'eval_id': '',\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe03',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'person',\n",
       "                'bounding_box': [\n",
       "                    0.645,\n",
       "                    0.3699765258215963,\n",
       "                    0.082890625,\n",
       "                    0.3239671361502347,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'person',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e37734011bb4c2e295a13b',\n",
       "                'eval_iou': 0.965604537205773,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe04',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'person',\n",
       "                'bounding_box': [\n",
       "                    0.600671875,\n",
       "                    0.40424882629107983,\n",
       "                    0.023625,\n",
       "                    0.08389671361502347,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'person',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'fn',\n",
       "                'eval_id': '',\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe05',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'microwave',\n",
       "                'bounding_box': [\n",
       "                    0.80034375,\n",
       "                    0.482981220657277,\n",
       "                    0.02303125,\n",
       "                    0.037488262910798126,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'appliance',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'fn',\n",
       "                'eval_id': '',\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe06',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'refrigerator',\n",
       "                'bounding_box': [\n",
       "                    0.77046875,\n",
       "                    0.40924882629107984,\n",
       "                    0.031703125,\n",
       "                    0.2542488262910798,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'appliance',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'fn',\n",
       "                'eval_id': '',\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe07',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'book',\n",
       "                'bounding_box': [\n",
       "                    0.944953125,\n",
       "                    0.7180516431924883,\n",
       "                    0.02240625,\n",
       "                    0.10730046948356808,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'indoor',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'fn',\n",
       "                'eval_id': '',\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe08',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'book',\n",
       "                'bounding_box': [\n",
       "                    0.9581875,\n",
       "                    0.7235680751173709,\n",
       "                    0.020125,\n",
       "                    0.10901408450704225,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'indoor',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'fn',\n",
       "                'eval_id': '',\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe09',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'clock',\n",
       "                'bounding_box': [\n",
       "                    0.699640625,\n",
       "                    0.2843192488262911,\n",
       "                    0.021828125,\n",
       "                    0.05136150234741784,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'indoor',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e37734011bb4c2e295a133',\n",
       "                'eval_iou': 0.776814119658313,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe0a',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'vase',\n",
       "                'bounding_box': [\n",
       "                    0.8579062499999999,\n",
       "                    0.7263615023474178,\n",
       "                    0.0573125,\n",
       "                    0.2104929577464789,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'indoor',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e37734011bb4c2e295a136',\n",
       "                'eval_iou': 0.8187941525834083,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe0b',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'vase',\n",
       "                'bounding_box': [\n",
       "                    0.5480625,\n",
       "                    0.4902347417840376,\n",
       "                    0.017765625,\n",
       "                    0.05293427230046949,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'indoor',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'fn',\n",
       "                'eval_id': '',\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe0c',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'chair',\n",
       "                'bounding_box': [\n",
       "                    0.644140625,\n",
       "                    0.514131455399061,\n",
       "                    0.015046875000000001,\n",
       "                    0.029389671361502348,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'furniture',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'fn',\n",
       "                'eval_id': '',\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe0d',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'vase',\n",
       "                'bounding_box': [\n",
       "                    0.37693750000000004,\n",
       "                    0.4577230046948357,\n",
       "                    0.022218750000000002,\n",
       "                    0.04138497652582159,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'indoor',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'fn',\n",
       "                'eval_id': '',\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe0e',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'vase',\n",
       "                'bounding_box': [\n",
       "                    0.526234375,\n",
       "                    0.46830985915492956,\n",
       "                    0.015203125000000001,\n",
       "                    0.039272300469483566,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'indoor',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'fn',\n",
       "                'eval_id': '',\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e3770d011bb4c2e294fe0f',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'dining table',\n",
       "                'bounding_box': [\n",
       "                    0.5018906249999999,\n",
       "                    0.5427699530516432,\n",
       "                    0.19618750000000001,\n",
       "                    0.20875586854460096,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'furniture',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e37734011bb4c2e295a132',\n",
       "                'eval_iou': 0.5078758627030245,\n",
       "            }>,\n",
       "        ],\n",
       "    }>,\n",
       "    'yolov8n_det_filepath': 'runs/detect/predict/labels/000000000139.txt',\n",
       "    'yolov8n': <Detections: {\n",
       "        'detections': [\n",
       "            <Detection: {\n",
       "                'id': '63e37734011bb4c2e295a131',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'chair',\n",
       "                'bounding_box': [0.63124975, 0.518779, 0.0640625, 0.201878],\n",
       "                'mask': None,\n",
       "                'confidence': 0.265856,\n",
       "                'index': None,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e3770d011bb4c2e294fe01',\n",
       "                'eval_iou': 0.6961515600731741,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e37734011bb4c2e295a132',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'dining table',\n",
       "                'bounding_box': [0.46406250000000004, 0.507042, 0.184375, 0.225352],\n",
       "                'mask': None,\n",
       "                'confidence': 0.266771,\n",
       "                'index': None,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e3770d011bb4c2e294fe0f',\n",
       "                'eval_iou': 0.5078758627030245,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e37734011bb4c2e295a133',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'clock',\n",
       "                'bounding_box': [0.7015629999999999, 0.2840378, 0.01875, 0.0469484],\n",
       "                'mask': None,\n",
       "                'confidence': 0.277868,\n",
       "                'index': None,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e3770d011bb4c2e294fe09',\n",
       "                'eval_iou': 0.776814119658313,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e37734011bb4c2e295a134',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'dining table',\n",
       "                'bounding_box': [0.7203125, 0.838028, 0.279687, 0.159624],\n",
       "                'mask': None,\n",
       "                'confidence': 0.278297,\n",
       "                'index': None,\n",
       "                'eval': 'fp',\n",
       "                'eval_id': '',\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e37734011bb4c2e295a135',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'refrigerator',\n",
       "                'bounding_box': [0.6937500000000001, 0.3920185, 0.101562, 0.295775],\n",
       "                'mask': None,\n",
       "                'confidence': 0.356417,\n",
       "                'index': None,\n",
       "                'eval': 'fp',\n",
       "                'eval_id': '',\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e37734011bb4c2e295a136',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'vase',\n",
       "                'bounding_box': [0.8578122499999999, 0.699531, 0.0609375, 0.241784],\n",
       "                'mask': None,\n",
       "                'confidence': 0.391675,\n",
       "                'index': None,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e3770d011bb4c2e294fe0a',\n",
       "                'eval_iou': 0.8187941525834083,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e37734011bb4c2e295a137',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'potted plant',\n",
       "                'bounding_box': [0.35312475, 0.4154925, 0.0640625, 0.084507],\n",
       "                'mask': None,\n",
       "                'confidence': 0.420693,\n",
       "                'index': None,\n",
       "                'eval': 'fp',\n",
       "                'eval_id': '',\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e37734011bb4c2e295a138',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'chair',\n",
       "                'bounding_box': [0.5640625, 0.511737, 0.090625, 0.21831],\n",
       "                'mask': None,\n",
       "                'confidence': 0.50562,\n",
       "                'index': None,\n",
       "                'eval': 'fp',\n",
       "                'eval_id': '',\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e37734011bb4c2e295a139',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'chair',\n",
       "                'bounding_box': [0.60781225, 0.511737, 0.0859375, 0.215962],\n",
       "                'mask': None,\n",
       "                'confidence': 0.508265,\n",
       "                'index': None,\n",
       "                'eval': 'fp',\n",
       "                'eval_id': '',\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e37734011bb4c2e295a13a',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'chair',\n",
       "                'bounding_box': [0.5656245, 0.5093894999999999, 0.128125, 0.220657],\n",
       "                'mask': None,\n",
       "                'confidence': 0.523211,\n",
       "                'index': None,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e3770d011bb4c2e294fdff',\n",
       "                'eval_iou': 0.576508295303288,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e37734011bb4c2e295a13b',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'person',\n",
       "                'bounding_box': [0.64531275, 0.3685445, 0.0828125, 0.333333],\n",
       "                'mask': None,\n",
       "                'confidence': 0.712339,\n",
       "                'index': None,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e3770d011bb4c2e294fe03',\n",
       "                'eval_iou': 0.965604537205773,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e37734011bb4c2e295a13c',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'chair',\n",
       "                'bounding_box': [\n",
       "                    0.45781274999999994,\n",
       "                    0.5093894999999999,\n",
       "                    0.0953125,\n",
       "                    0.230047,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': 0.854189,\n",
       "                'index': None,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e3770d011bb4c2e294fe00',\n",
       "                'eval_iou': 0.9173482595434582,\n",
       "            }>,\n",
       "            <Detection: {\n",
       "                'id': '63e37734011bb4c2e295a13d',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'tv',\n",
       "                'bounding_box': [0.009374999999999994, 0.389671, 0.23125, 0.225352],\n",
       "                'mask': None,\n",
       "                'confidence': 0.927385,\n",
       "                'index': None,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e3770d011bb4c2e294fdfd',\n",
       "                'eval_iou': 0.9536521929850259,\n",
       "            }>,\n",
       "        ],\n",
       "    }>,\n",
       "    'eval_tp': 8,\n",
       "    'eval_fp': 5,\n",
       "    'eval_fn': 12,\n",
       "}>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac72daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.clone()\n",
    "dataset.name = \"coco_val_eval\"\n",
    "dataset.persistent = True\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ea26579",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_patches = dataset.to_evaluation_patches(\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfb4baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.delete_sample_field(\"ground_truth.detections.mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20f6c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_view(\"eval_patches\", eval_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d96839b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "       person       0.85      0.68      0.76     11573\n",
      "          car       0.71      0.52      0.60      1971\n",
      "        chair       0.62      0.34      0.44      1806\n",
      "         book       0.61      0.12      0.20      1182\n",
      "       bottle       0.68      0.39      0.50      1051\n",
      "          cup       0.61      0.44      0.51       907\n",
      " dining table       0.54      0.42      0.47       697\n",
      "traffic light       0.66      0.36      0.46       638\n",
      "         bowl       0.63      0.49      0.55       636\n",
      "      handbag       0.48      0.12      0.19       540\n",
      "         bird       0.79      0.39      0.52       451\n",
      "         boat       0.58      0.29      0.39       430\n",
      "        truck       0.57      0.35      0.44       415\n",
      "        bench       0.58      0.27      0.37       413\n",
      "     umbrella       0.65      0.52      0.58       423\n",
      "          cow       0.81      0.61      0.70       397\n",
      "       banana       0.68      0.34      0.45       397\n",
      "       carrot       0.56      0.29      0.38       384\n",
      "   motorcycle       0.77      0.58      0.66       379\n",
      "     backpack       0.51      0.16      0.24       371\n",
      "\n",
      "    micro avg       0.76      0.52      0.61     25061\n",
      "    macro avg       0.64      0.38      0.47     25061\n",
      " weighted avg       0.74      0.52      0.60     25061\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts = dataset.count_values(\"ground_truth.detections.label\")\n",
    "top20_classes = sorted(counts, key=counts.get, reverse=True)[:20]\n",
    "detection_results.print_report(classes=top20_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22320197",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dat = dataset.select_fields(\"ground_truth\")\n",
    "dataset.save_view(\"coco_validation_detections\", gt_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a65cad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4803ad27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to '/scratch/user/jacob/zoo_datasets/coco-2017/validation' if necessary\n",
      "Found annotations at '/scratch/user/jacob/zoo_datasets/coco-2017/raw/instances_val2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading existing dataset 'coco-2017-validation'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "seg_dataset = foz.load_zoo_dataset(\n",
    "    'coco-2017',\n",
    "    split='validation',\n",
    "    label_types=\"segmentations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a691e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        coco-2017-validation\n",
       "Media type:  image\n",
       "Num samples: 5000\n",
       "Persistent:  False\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:                   fiftyone.core.fields.ObjectIdField\n",
       "    filepath:             fiftyone.core.fields.StringField\n",
       "    tags:                 fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:             fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    ground_truth:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    yolov8n_det_filepath: fiftyone.core.fields.StringField\n",
       "    yolov8n:              fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    eval_tp:              fiftyone.core.fields.IntField\n",
       "    eval_fp:              fiftyone.core.fields.IntField\n",
       "    eval_fn:              fiftyone.core.fields.IntField"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f615f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73a3541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719af10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb51198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8849ca1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4268d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99801fe4-46b5-48c7-978e-8545a3b8125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# birds_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ede522bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "birds_test_dataset = dataset.filter_labels(\n",
    "    \"ground_truth\", \n",
    "    F(\"label\") == \"bird\"\n",
    ").filter_labels(\n",
    "    \"yolov8n\", \n",
    "    F(\"label\") == \"bird\",\n",
    "    only_matches=False\n",
    ").clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a58db000-e3e9-4683-a067-49c91a7dcf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |█████████████████| 125/125 [886.0ms elapsed, 0s remaining, 141.1 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |█████████████████| 125/125 [619.1ms elapsed, 0s remaining, 201.9 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "base_bird_results = birds_test_dataset.evaluate_detections(\n",
    "    \"yolov8n\", \n",
    "    eval_key=\"base\",\n",
    "    compute_mAP=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c77e370-38c0-45f2-8a1c-6320ae3b622d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bird       0.87      0.39      0.54       451\n",
      "\n",
      "   micro avg       0.87      0.39      0.54       451\n",
      "   macro avg       0.87      0.39      0.54       451\n",
      "weighted avg       0.87      0.39      0.54       451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_bird_results.print_report(classes=[\"bird\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36ee3e3e-9db0-4f6c-a75b-8343e126e5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24897924786479841"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_bird_results.mAP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677fde6b-1609-4814-aaf2-b6433a2a7a59",
   "metadata": {},
   "source": [
    "This is slightly higher because we took out images with FP bird detections!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d355356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "birds_test_dataset.name = \"birds-test-dataset\"\n",
    "birds_test_dataset.persistent = True\n",
    "birds_test_dataset.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d1091-39da-43aa-a0a2-87be9d437cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cba3c89",
   "metadata": {},
   "source": [
    "## Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "97d5b652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to '/scratch/user/jacob/zoo_datasets/coco-2017/train' if necessary\n",
      "Found annotations at '/scratch/user/jacob/zoo_datasets/coco-2017/raw/instances_train2017.json'\n",
      "Downloading 3237 images\n",
      " 100% |████████████████| 3237/3237 [1.2m elapsed, 0s remaining, 33.0 images/s]      \n",
      "Writing annotations for 3237 downloaded samples to '/scratch/user/jacob/zoo_datasets/coco-2017/train/labels.json'\n",
      "Dataset info written to '/scratch/user/jacob/zoo_datasets/coco-2017/info.json'\n",
      "Loading 'coco-2017' split 'train'\n",
      " 100% |███████████████| 3237/3237 [10.3s elapsed, 0s remaining, 298.8 samples/s]      \n",
      "Dataset 'coco-2017-train' created\n"
     ]
    }
   ],
   "source": [
    "train_dataset = foz.load_zoo_dataset(\n",
    "    'coco-2017',\n",
    "    split='train',\n",
    "    classes=[\"bird\"]\n",
    ").clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "89ca60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fo.delete_dataset(\"open-images-v6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9597fee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to '/scratch/user/jacob/zoo_datasets/open-images-v6/train' if necessary\n",
      "Necessary images already downloaded\n",
      "Existing download of split 'train' is sufficient\n",
      "Downloading split 'test' to '/scratch/user/jacob/zoo_datasets/open-images-v6/test' if necessary\n",
      "Necessary images already downloaded\n",
      "Existing download of split 'test' is sufficient\n",
      "Downloading split 'validation' to '/scratch/user/jacob/zoo_datasets/open-images-v6/validation' if necessary\n",
      "Necessary images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading 'open-images-v6' split 'train'\n",
      " 100% |█████████████| 18525/18525 [44.3s elapsed, 0s remaining, 458.2 samples/s]      \n",
      "Loading 'open-images-v6' split 'test'\n",
      " 100% |███████████████| 1871/1871 [3.2s elapsed, 0s remaining, 532.9 samples/s]      \n",
      "Loading 'open-images-v6' split 'validation'\n",
      " 100% |█████████████████| 593/593 [857.8ms elapsed, 0s remaining, 691.3 samples/s]      \n",
      "Dataset 'open-images-v6' created\n"
     ]
    }
   ],
   "source": [
    "oi_samples = foz.load_zoo_dataset(\n",
    "    \"open-images-v6\",\n",
    "    classes = [\"Bird\"],\n",
    "    only_matching=True,\n",
    "    label_types=\"detections\"\n",
    ").map_labels(\n",
    "    \"ground_truth\",\n",
    "    {\"Bird\":\"bird\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186bf46-a3b0-4369-93a0-6cd8503b0997",
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6fe44991-3780-43da-8983-45d45976aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oi_samples.values(\"ground_truth.detections.label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e0fb3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.merge_samples(oi_samples)\n",
    "train_dataset.untag_samples(train_dataset.distinct(\"tags\"))\n",
    "train_dataset.name = \"birds-train-data\"\n",
    "train_dataset.persistent = True\n",
    "train_dataset.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f96609cd-3463-402f-913f-e5f92c1134e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = fo.load_dataset(\"birds-train-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5662ea8a-0b9b-46b1-945f-eb4eb91a3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18525 + 1871 + 593  + 3237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef820601-376e-4c55-9ba9-c0a4620e6952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = train_dataset.filter_labels(\"ground_truth\", F(\"label\") == \"bird\").clone()\n",
    "# train_dataset.name = \"birds-train-data\"\n",
    "# train_dataset.persistent = True\n",
    "# train_dataset.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d515529-efaa-43bf-83ea-b4ff25a54033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb25641-b9c2-4894-b9d2-a3b36025b8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91623306-0ea8-4f3b-b64c-bf8330019299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef096092-f03c-4b8d-8150-6711afc2521e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6c88f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.random as four\n",
    "four.random_split(\n",
    "    train_dataset,\n",
    "    {\"train\": 0.8, \"val\": 0.2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "086482a5-ee5d-4486-b59b-f55099a5940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 19381/19381 [24.0s elapsed, 0s remaining, 1.1K samples/s]       \n",
      "Directory '/scratch/user/jacob/yolov8/only_birds' already exists; export will be merged with existing files\n",
      " 100% |███████████████| 4845/4845 [5.8s elapsed, 0s remaining, 872.1 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "export_yolo_data(train_dataset, \"/scratch/user/jacob/yolov8/only_birds\", [\"bird\"], split = [\"train\", \"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87e39118-145b-46a2-b35b-4d2677352ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.30 🚀 Python-3.9.13 torch-1.13.1 CUDA:0 (NVIDIA TITAN V, 12067MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/scratch/user/jacob/yolov8/only_birds/dataset.yaml, epochs=100, patience=50, batch=16, imgsz=640, save=True, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, overlap_mask=True, mask_ratio=4, dropout=False, val=True, save_json=False, save_hybrid=False, conf=0.001, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=ultralytics/assets/, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, save_dir=runs/detect/train\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.Detect                [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011043 parameters, 3011027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.001), 63 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /scratch/user/jacob/yolov8/only_birds/labels/train... 19381 imag\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /scratch/user/jacob/yolov8/only_birds/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /scratch/user/jacob/yolov8/only_birds/labels/val... 4845 images, 0\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /scratch/user/jacob/yolov8/only_birds/labels/val.cache\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      1/100      6.65G      1.392      1.627      1.345         22        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.677      0.524      0.581      0.339\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      2/100      9.58G      1.446      1.407      1.395         30        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.669       0.47       0.54      0.316\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      3/100      9.58G       1.54      1.493      1.462         29        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.529      0.329      0.349      0.188\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      4/100      9.59G      1.572       1.52      1.493         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487       0.67       0.48      0.547      0.312\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      5/100      9.59G      1.532      1.433      1.478         19        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.646      0.462      0.523        0.3\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      6/100      9.59G      1.502      1.369      1.447         12        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.657      0.513      0.566      0.325\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      7/100      9.59G      1.468       1.32      1.433         20        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.696      0.519       0.59      0.343\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      8/100      9.59G      1.457      1.293      1.417         33        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.701      0.528        0.6      0.352\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      9/100      9.59G      1.437       1.26      1.403         26        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.714      0.545      0.615      0.368\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     10/100      9.59G      1.429      1.241      1.401         28        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.722      0.538       0.62      0.375\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     11/100      9.59G      1.417       1.21       1.39         18        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.703      0.561      0.631       0.38\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     12/100      9.59G      1.409      1.202      1.385          9        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.731      0.559      0.635      0.382\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     13/100      9.59G      1.402      1.187      1.375         14        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.715      0.575      0.646      0.391\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     14/100      9.59G      1.386      1.177      1.371         11        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.731      0.568      0.647      0.397\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     15/100      9.59G      1.393      1.171      1.368         12        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.719      0.589      0.659      0.402\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     16/100      9.59G       1.38      1.156      1.365         31        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.712      0.582      0.653      0.402\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     17/100      9.59G      1.373      1.146      1.357         26        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.731      0.587      0.659      0.402\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     18/100      9.59G      1.363      1.135      1.352         13        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.731      0.598      0.667      0.407\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     19/100      9.59G      1.362       1.13      1.355         17        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.733      0.592      0.666       0.41\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     20/100      9.59G      1.363      1.117      1.347         27        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.748      0.588       0.67      0.412\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     21/100      9.59G      1.352      1.103      1.338         27        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.737        0.6      0.675      0.416\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     22/100      9.59G      1.356      1.108      1.341         32        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.732        0.6      0.673      0.417\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     23/100      9.59G      1.345      1.097       1.34         11        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.737      0.602      0.678      0.419\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     24/100      9.59G      1.343      1.095      1.341         20        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.741      0.603      0.684      0.423\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     25/100      9.59G      1.339      1.088      1.332         32        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.739      0.611      0.684      0.423\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     26/100      9.59G       1.34      1.079      1.329         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.738      0.616      0.688      0.428\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     27/100      9.59G      1.339      1.073       1.33        138        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.745      0.608      0.686      0.428\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     28/100      9.59G      1.338      1.074      1.329         21        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.743       0.62      0.691      0.431\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     29/100      9.59G      1.335      1.069      1.323         12        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487       0.74      0.621      0.694      0.432\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     30/100      9.59G      1.324      1.061      1.325         24        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.749      0.615      0.694      0.432\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     31/100      9.59G      1.322      1.051      1.321         55        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.755      0.608      0.692      0.432\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     32/100      9.59G      1.322      1.049      1.317         12        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.751      0.613      0.694      0.433\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     33/100      9.59G      1.321      1.041      1.319         19        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.749      0.614      0.696      0.433\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     34/100      9.59G      1.314      1.041      1.317         26        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.751      0.618      0.699      0.435\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     35/100      9.59G      1.313      1.032      1.313         21        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.756      0.617        0.7      0.436\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     36/100      9.59G      1.318      1.034      1.312         23        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.759      0.613      0.698      0.437\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     37/100      9.59G      1.313      1.028      1.306         19        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.762       0.61      0.699      0.439\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     38/100      9.59G      1.307      1.031       1.31         20        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.759      0.613        0.7      0.439\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     39/100      9.59G      1.307       1.02      1.305         21        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.752      0.624      0.699       0.44\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     40/100      9.59G      1.307      1.013      1.305         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.753      0.622      0.701       0.44\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     41/100      9.59G      1.302      1.016      1.304         14        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.751      0.627      0.702      0.441\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     42/100      9.59G      1.302      1.008      1.302          8        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.749      0.623      0.703      0.441\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     43/100      9.59G      1.297      1.004        1.3         36        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.749      0.624      0.703      0.441\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     44/100      9.59G      1.295      1.006      1.298         13        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.748      0.627      0.703      0.441\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     45/100      9.59G      1.293      1.002      1.303         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.748      0.628      0.703      0.442\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     46/100      9.59G      1.297     0.9921      1.295         10        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.749      0.629      0.703      0.442\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     47/100      9.59G       1.29     0.9963      1.291         12        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.752      0.627      0.705      0.443\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     48/100      9.59G      1.286     0.9838      1.292         26        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.752      0.629      0.705      0.444\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     49/100      9.59G      1.278     0.9778      1.288         22        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.753      0.628      0.706      0.444\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     50/100      9.59G      1.282     0.9847      1.294         15        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.749      0.632      0.707      0.444\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     51/100      9.59G      1.279     0.9763      1.287         30        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.751      0.631      0.706      0.444\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     52/100      9.59G      1.275     0.9792      1.286         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.753      0.628      0.706      0.444\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     53/100      9.59G      1.277     0.9719      1.288         23        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.754      0.628      0.707      0.445\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     54/100      9.59G      1.278     0.9618      1.285         14        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.754      0.629      0.707      0.445\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     55/100      9.59G      1.268     0.9595      1.282         22        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.751      0.631      0.707      0.445\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     56/100      9.59G      1.272     0.9609      1.281         26        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.752      0.632      0.708      0.445\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     57/100      9.59G      1.268     0.9547      1.278         17        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.753      0.629      0.707      0.446\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     58/100      9.59G      1.263     0.9489      1.277         47        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.751      0.631      0.708      0.446\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     59/100      9.59G      1.264     0.9476      1.277         29        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.752      0.631      0.708      0.446\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     60/100      9.59G      1.257     0.9456      1.274         41        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       4845      12487      0.752      0.631      0.709      0.446\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     61/100      9.59G      1.228     0.9513      1.263         55        640:  ^C\n",
      "     61/100      9.59G      1.228     0.9487      1.267         43        640:  \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/bin/yolo\", line 8, in <module>\n",
      "    sys.exit(entrypoint())\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/site-packages/ultralytics/yolo/cfg/__init__.py\", line 266, in entrypoint\n",
      "    getattr(model, mode)(**overrides)  # default args from model\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/site-packages/ultralytics/yolo/engine/model.py\", line 214, in train\n",
      "    self.trainer.train()\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/site-packages/ultralytics/yolo/engine/trainer.py\", line 185, in train\n",
      "    self._do_train(int(os.getenv(\"RANK\", -1)), world_size)\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/site-packages/ultralytics/yolo/engine/trainer.py\", line 311, in _do_train\n",
      "    self.scaler.scale(self.loss).backward()\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "Exception in thread Thread-210:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/site-packages/sentry_sdk/integrations/threading.py\", line 69, in run\n",
      "    reraise(*_capture_exception())\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/site-packages/sentry_sdk/_compat.py\", line 60, in reraise\n",
      "    raise value\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/site-packages/sentry_sdk/integrations/threading.py\", line 67, in run\n",
      "    return old_run_func(self, *a, **kw)\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py\", line 49, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py\", line 26, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 305, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/multiprocessing/connection.py\", line 513, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/multiprocessing/connection.py\", line 757, in answer_challenge\n",
      "    message = connection.recv_bytes(256)         # reject large message\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/multiprocessing/connection.py\", line 221, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/jacob/miniconda3/envs/foenv/lib/python3.9/multiprocessing/connection.py\", line 388, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=train model=yolov8n.pt data=/scratch/user/jacob/yolov8/only_birds/dataset.yaml epochs=100 imgsz=640 batch=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db7415fd-ba87-42dc-9f88-9e08e786ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "birds_test_dataset = fo.load_dataset('birds-test-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4569adc0-5612-4fdb-a4e2-c497b82db8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 125/125 [297.4ms elapsed, 0s remaining, 420.3 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "export_yolo_data(birds_test_dataset, \"/scratch/user/jacob/yolov8/birds_test\", [\"bird\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7758df2e-2ff2-48aa-a70e-facb698df682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.30 🚀 Python-3.9.13 torch-1.13.1 CUDA:0 (NVIDIA TITAN V, 12067MiB)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
      "114 labels saved to runs/detect/predict2/labels\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=predict model=runs/detect/train/weights/best.pt  source=/scratch/user/jacob/yolov8/birds_test/images/val save_txt=True save_conf=True verbose=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e390039-0a32-47aa-8366-ccc11adee1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = birds_test_dataset.values(\"filepath\")\n",
    "prediction_filepaths = [get_prediction_filepath(fp, run_number=2) for fp in filepaths]\n",
    "birds_test_dataset.set_values(\"yolov8n_bird_det_filepath\", prediction_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a91d34a-34fa-477c-99dc-0f53d9090863",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_yolo_detections(\n",
    "    birds_test_dataset, \n",
    "    \"yolov8n_bird\", \n",
    "    \"yolov8n_bird_det_filepath\", \n",
    "    [\"bird\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "758c5e7f-678b-4c7b-9582-c694a25cc1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.yaml  \u001b[0m\u001b[01;34mimages\u001b[0m/  \u001b[01;34mlabels\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /scratch/user/jacob/yolov8/birds_test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f0da44f5-fa81-4319-af8a-b380f6dec3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sample: {\n",
       "    'id': '63e2cab2b38812bc02a03f97',\n",
       "    'media_type': 'image',\n",
       "    'filepath': '/scratch/user/jacob/zoo_datasets/coco-2017/validation/data/000000001268.jpg',\n",
       "    'tags': ['validation'],\n",
       "    'metadata': <ImageMetadata: {\n",
       "        'size_bytes': None,\n",
       "        'mime_type': None,\n",
       "        'width': 640,\n",
       "        'height': 427,\n",
       "        'num_channels': None,\n",
       "    }>,\n",
       "    'ground_truth': <Detections: {\n",
       "        'detections': [\n",
       "            <Detection: {\n",
       "                'id': '63e2cab2b38812bc02a03f7a',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'bird',\n",
       "                'bounding_box': [\n",
       "                    0.301265625,\n",
       "                    0.5264637002341921,\n",
       "                    0.11676562500000001,\n",
       "                    0.0782903981264637,\n",
       "                ],\n",
       "                'mask': None,\n",
       "                'confidence': None,\n",
       "                'index': None,\n",
       "                'supercategory': 'animal',\n",
       "                'iscrowd': 0,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e2cb9fb38812bc02a0e290',\n",
       "                'eval_iou': 0.8433369003373558,\n",
       "                'base': 'tp',\n",
       "                'base_id': '63e2cb9fb38812bc02a0e290',\n",
       "                'base_iou': 0.8433369003373558,\n",
       "            }>,\n",
       "        ],\n",
       "    }>,\n",
       "    'yolov8n_det_filepath': 'runs/detect/predict/labels/000000001268.txt',\n",
       "    'yolov8n': <Detections: {\n",
       "        'detections': [\n",
       "            <Detection: {\n",
       "                'id': '63e2cb9fb38812bc02a0e290',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'bird',\n",
       "                'bounding_box': [0.296875, 0.5292743, 0.1125, 0.0772834],\n",
       "                'mask': None,\n",
       "                'confidence': 0.537588,\n",
       "                'index': None,\n",
       "                'eval': 'tp',\n",
       "                'eval_id': '63e2cab2b38812bc02a03f7a',\n",
       "                'eval_iou': 0.8433369003373558,\n",
       "                'base': 'tp',\n",
       "                'base_id': '63e2cab2b38812bc02a03f7a',\n",
       "                'base_iou': 0.8433369003373558,\n",
       "            }>,\n",
       "        ],\n",
       "    }>,\n",
       "    'eval_tp': 7,\n",
       "    'eval_fp': 3,\n",
       "    'eval_fn': 4,\n",
       "    'base_tp': 1,\n",
       "    'base_fp': 0,\n",
       "    'base_fn': 0,\n",
       "    'yolov8n_bird_det_filepath': 'runs/detect/predict2/labels/000000001268.txt',\n",
       "    'yolov8n_bird': <Detections: {\n",
       "        'detections': [\n",
       "            <Detection: {\n",
       "                'id': '63e2f9c11c52411e939325ef',\n",
       "                'attributes': {},\n",
       "                'tags': [],\n",
       "                'label': 'bird',\n",
       "                'bounding_box': [0.3015625, 0.52693235, 0.104687, 0.0796253],\n",
       "                'mask': None,\n",
       "                'confidence': 0.720047,\n",
       "                'index': None,\n",
       "            }>,\n",
       "        ],\n",
       "    }>,\n",
       "}>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birds_test_dataset.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b8be56d6-1f49-4c9e-9cfc-aa683835c2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |█████████████████| 125/125 [954.4ms elapsed, 0s remaining, 131.0 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |█████████████████| 125/125 [751.8ms elapsed, 0s remaining, 166.3 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "finetune_bird_results = birds_test_dataset.evaluate_detections(\n",
    "    \"yolov8n_bird\", \n",
    "    eval_key=\"finetune\",\n",
    "    compute_mAP=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e601002b-5956-49e3-8ac7-ddcf53a97d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bird       0.81      0.56      0.67       506\n",
      "\n",
      "   micro avg       0.81      0.56      0.67       506\n",
      "   macro avg       0.81      0.56      0.67       506\n",
      "weighted avg       0.81      0.56      0.67       506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finetune_bird_results.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e511b08-5baa-4af0-ad70-71dd1b6e5889",
   "metadata": {},
   "outputs": [],
   "source": [
    "birds_test_dataset.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "79e559b6-7235-495b-924c-49eea15c394e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31339033693212076"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_bird_results.mAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d4a653-aa14-45fb-832f-2d5381caaa60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
