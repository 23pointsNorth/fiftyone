{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection Mistakenness with FiftyOne\n",
    "\n",
    "Finding mistakes in your annotations can be extremely tedious. The mistakenness feature of FiftyOne can be used to help you find annotation mistakes. Check out [our classification tutorial](https://voxel51.com/docs/fiftyone/tutorials/label_mistakes.html) to see how FiftyOne can help you find and correct label mistakes in your classification datasets.\n",
    "\n",
    "This recipe demonstrates the use of the FiftyOne Brain's [mistakenness method](https://voxel51.com/docs/fiftyone/user_guide/brain.html#label-mistakes) on your detection dataset, enabling you to curate higher quality datasets and, ultimately, train better models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this recipe, we explore how FiftyOne can be used to help you find mistakes in your detection annotations using the FiftyOne Brain's [mistakenness method](https://voxel51.com/docs/fiftyone/user_guide/brain.html#label-mistakes).\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "- A detection model trained on the same label schema as the annotations you want to analyze\n",
    "- A FiftyOne Dataset with your annotations and predictions with confidence from the model (with optional logits for each detection)\n",
    "- It is recommended to use logits for each prediction, however, since these can be hard to come by you can compute mistakenness with just confidence for each prediction by passing the `use_logits = False` argument when calling `compute_mistakenness()`\n",
    "\n",
    "**Concepts**\n",
    "\n",
    "-   Computing insights into your detection dataset relating to possible mistakes\n",
    "-   Visualizing mistakes in the FiftyOne App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Your Dataset should have two `Detections` fields, one with your ground truth annotations and one with your model predictions.\n",
    "\n",
    "In this example, we'll load the `quickstart` dataset from the [FiftyOne Dataset Zoo](https://voxel51.com/docs/fiftyone/user_guide/dataset_creation/zoo.html), which has ground truth annotations and predictions from a [PyTorch Faster-RCNN model](https://github.com/pytorch/vision/blob/master/torchvision/models/detection/faster_rcnn.py) for a few samples from the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded\n",
      "Loading 'quickstart'\n",
      " 100% |████████████████████████████████████████| 200/200 [3.4s elapsed, 0s remaining, 44.0 samples/s]      \n",
      "App launched\n",
      "Name:           quickstart\n",
      "Media type:     image\n",
      "Num samples:    200\n",
      "Persistent:     False\n",
      "Info:           {}\n",
      "Tags:           ['validation']\n",
      "Sample fields:\n",
      "    filepath:     fiftyone.core.fields.StringField\n",
      "    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n",
      "    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    uniqueness:   fiftyone.core.fields.FloatField\n",
      "    predictions:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "dataset, session = fo.quickstart()\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Detection: {\n",
      "    'id': '5f452c60ef00e6374aad9394',\n",
      "    '_id': '5f452c60ef00e6374aad9394',\n",
      "    'attributes': BaseDict({}),\n",
      "    'label': 'bird',\n",
      "    'bounding_box': BaseList([\n",
      "        0.22192673683166503,\n",
      "        0.06093006531397502,\n",
      "        0.4808845520019531,\n",
      "        0.8937615712483724,\n",
      "    ]),\n",
      "    'mask': None,\n",
      "    'confidence': 0.9750854969024658,\n",
      "    'index': None,\n",
      "    '_cls': 'Detection',\n",
      "}>\n"
     ]
    }
   ],
   "source": [
    "print(dataset.first().predictions.detections[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute mistakenness\n",
    "\n",
    "Now we're ready to assess the mistakenness of the ground truth detections.\n",
    "\n",
    "We can do so by running the [compute_uniqueness()](https://voxel51.com/docs/fiftyone/api/fiftyone.brain.html#fiftyone.brain.compute_mistakenness) method from the FiftyOne Brain.\n",
    "\n",
    "> **NOTE:** It is recommended to add logits to each detection. However, if your dataset doesn't have them\n",
    "> you can specify `use_logits = False`. Since we don't have logits in this example, we'll do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |██████████████████| 200/200 [4.2s elapsed, 0s remaining, 35.3 samples/s]      \n",
      "Computing mistakenness...\n",
      " 100% |██████████████████| 200/200 [3.8s elapsed, 0s remaining, 42.8 samples/s]      \n",
      "Mistakenness computation complete\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "# Compute mistakenness, use the field names of your dataset in place of \"predictions\" and \"ground_truth\"\n",
    "fob.compute_mistakenness(dataset, \"predictions\", label_field=\"ground_truth\", use_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method added fields to all samples for which we had predictions at both a sample and detection level. Specifically, it added the following.\n",
    "\n",
    "Ground truth detections (`ground_truth` field):\n",
    "\n",
    "- `mistakenness`: A measure of the correctness of the detection and classification of the object therein.(When ground truth and predicted detections are matched)\n",
    "- `mistakenness_loc`: A measure of the mistakenness of the localization (bounding box) of a ground truth object. (When ground truth and predicted detections are matched)\n",
    "- `possible_spurious`: If the ground truth object was not matched with a prediction, it is flagged as a possible mistake\n",
    "\n",
    "New ground truth predictions (`ground_truth` field):\n",
    "\n",
    "- `possible_missing`: If a highly confidence prediction with no matching ground truth object is encountered, it is added to the ground truth objects with this flag populated to indicate that it is a likely missing annotation\n",
    "\n",
    "Sample-level fields:\n",
    "\n",
    "- `mistakenness`: The maximum mistakenness of all ground truth detections with matching predictions\n",
    "- `possible_spurious`: The number of possible spurious detections across the ground truth annotations in the sample\n",
    "- `possible_missing`: The number of possible missing detections deduced and added to the  all highly confident predictions in the sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the results\n",
    "\n",
    "Let's use FiftyOne to investigate the results.\n",
    "\n",
    "First, let's show the samples with the most likely annotation mistakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:        quickstart\n",
      "Media type:     image\n",
      "Num samples:    200\n",
      "Tags:           ['validation']\n",
      "Sample fields:\n",
      "    filepath:          fiftyone.core.fields.StringField\n",
      "    tags:              fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:          fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n",
      "    ground_truth:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    uniqueness:        fiftyone.core.fields.FloatField\n",
      "    predictions:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    mistakenness:      fiftyone.core.fields.FloatField\n",
      "    possible_missing:  fiftyone.core.fields.IntField\n",
      "    possible_spurious: fiftyone.core.fields.IntField\n",
      "Pipeline stages:\n",
      "    1. SortBy(field_or_expr='mistakenness', reverse=True)\n"
     ]
    }
   ],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Sort by likelihood of mistake (most likely first)\n",
    "mistake_view = (dataset\n",
    "    .sort_by(\"mistakenness\", reverse=True)\n",
    ")\n",
    "\n",
    "# Print some information about the view\n",
    "print(mistake_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Detection: {\n",
      "    'id': '5f452487ef00e6374aad2744',\n",
      "    '_id': '5f452487ef00e6374aad2744',\n",
      "    'attributes': BaseDict({\n",
      "        'area': <NumericAttribute: {'value': 16273.3536, '_cls': 'NumericAttribute'}>,\n",
      "        'iscrowd': <NumericAttribute: {'value': 0.0, '_cls': 'NumericAttribute'}>,\n",
      "    }),\n",
      "    'label': 'tv',\n",
      "    'bounding_box': BaseList([\n",
      "        0.002746666666666667,\n",
      "        0.36082,\n",
      "        0.24466666666666667,\n",
      "        0.3732,\n",
      "    ]),\n",
      "    'mask': None,\n",
      "    'confidence': None,\n",
      "    'index': None,\n",
      "    '_cls': 'Detection',\n",
      "    'predictions_eval': BaseDict({\n",
      "        'matches': BaseDict({\n",
      "            '0_5': BaseDict({\n",
      "                'pred_id': '5f452c65ef00e6374aadd6c2',\n",
      "                'iou': 0.9171496791329676,\n",
      "            }),\n",
      "        }),\n",
      "    }),\n",
      "    'mistakenness_loc': 0.16955941131917984,\n",
      "    'mistakenness': 0.005771428346633911,\n",
      "}>\n"
     ]
    }
   ],
   "source": [
    "# Inspect some samples and detections\n",
    "# This is the first detection of the first sample\n",
    "print(mistake_view.first().ground_truth.detections[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the App to visually inspect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open your dataset in the App\n",
    "session.dataset = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset](images/det_mistakenness_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the samples we processed in rank order by the mistakenness\n",
    "session.view = mistake_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![view](images/det_mistakenness_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful query is to find all objects that have a high mistakenness, lets say > 0.95:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "high_mistake_view = dataset.filter_detections(\"ground_truth\", F(\"mistakenness\") > 0.95, only_matches=True)\n",
    "\n",
    "session.view = high_mistake_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking through the results, we see some annotations that may be incorrect. For example, in the image below the `goat` is labeled as a `sheep`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sheep](images/det_mistakenness_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a similar workflow to look at objects that may be localized poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_view = dataset.filter_detections(\"ground_truth\", F(\"mistakenness_loc\") > 0.95, only_matches=True)\n",
    "\n",
    "session.view = loc_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the examples that popped up from this query is shown below. The bounding box around the person on the left side of the image is shifted too far to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![wine](images/det_mistakenness_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `possible_missing` field can also be useful to sort by to find instances of incorrect annotations. Similarly, `possible_spurious` can be used to find objects that the model detected that may have been missed by annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.view = dataset.match(F(\"possible_missing\") > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example that showed up from this search is shown below. There is an `apple` that was not annotated that the model detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![car](images/det_mistakenness_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **REMEMBER**: Since you are using model predictions to guide the mistakenness process, the better your model,\n",
    "> the more accurate the mistakenness suggestions. Additionally, using logits of confidence scores will also\n",
    "> provide better results. \n",
    "\n",
    "We used Faster-RCNN in this example which is quite a few years old. Using EfficientDet D7 provided much better results. For example, it was easily able to find this `snowboard` labeled as `skis`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![skis](images/det_mistakenness_6.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
