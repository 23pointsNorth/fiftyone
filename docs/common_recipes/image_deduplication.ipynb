{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Note\n",
    "    \n",
    "![Voxel51 Icon](../images/voxel51_32x32.png) [View on Voxel51.com](https://voxel51.com/docs/fiftyone/common_recipes/image_deduplication.html)\n",
    "\n",
    "![Github Icon](../images/GitHub-Mark-32px.png) [View source on GitHub](https://github.com/voxel51/fiftyone/tree/develop/docs/common_recipes/image_deduplication.ipynb)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Deduplication\n",
    "\n",
    "This walkthrough demonstrates a simple use case of using FiftyOne to detect and\n",
    "remove duplicate images from your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "This walkthrough requires the `tensorflow` package.\n",
    "\n",
    "```bash\n",
    "pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "\n",
    "First we download the dataset to disk. The dataset is a 1000 sample subset of\n",
    "CIFAR-100, a dataset of 32x32 pixel images with one of 100 different\n",
    "classification labels such as `apple`, `bicycle`, `porcupine`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_deduplication_helpers import download_dataset\n",
    "\n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above script uses `tensorflow.keras.datasets` to download the dataset, so\n",
    "you must have [TensorFlow installed](https://www.tensorflow.org/install).\n",
    "\n",
    "The dataset is organized on disk as follows:\n",
    "\n",
    "```\n",
    "/tmp/fiftyone/\n",
    "└── cifar100_with_duplicates/\n",
    "    ├── <classA>/\n",
    "    │   ├── <image1>.jpg\n",
    "    │   ├── <image2>.jpg\n",
    "    │   └── ...\n",
    "    ├── <classB>/\n",
    "    │   ├── <image1>.jpg\n",
    "    │   ├── <image2>.jpg\n",
    "    │   └── ...\n",
    "    └── ...\n",
    "```\n",
    "\n",
    "As we will soon come to discover, some of these samples are duplicates and we\n",
    "have no clue which they are!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset\n",
    "\n",
    "First import the `fiftyone` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a utililty method provided by FiftyOne to load the image\n",
    "classification dataset from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import fiftyone.utils.data as foud\n",
    "\n",
    "dataset_name = \"cifar100_with_duplicates\"\n",
    "\n",
    "src_data_dir = os.path.join(\"/tmp/fiftyone\", dataset_name)\n",
    "\n",
    "samples, classes = foud.parse_image_classification_dir_tree(src_data_dir)\n",
    "dataset = fo.Dataset.from_image_classification_samples(\n",
    "    samples, name=dataset_name, classes=classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataset\n",
    "\n",
    "We can poke around in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary information about the dataset\n",
    "print(dataset)\n",
    "\n",
    "# Print a random sample\n",
    "print(dataset.view().take(1).first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a view that contains only samples whose ground truth label is\n",
    "`mountain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = dataset.view().match({\"ground_truth.label\": \"mountain\"})\n",
    "\n",
    "# Print summary information about the view\n",
    "print(view)\n",
    "\n",
    "# Print the first sample in the view\n",
    "print(view.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a view with samples sorted by their ground truth labels in reverse\n",
    "alphabetical order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = dataset.view().sort_by(\"ground_truth.label\", reverse=True)\n",
    "\n",
    "print(view)\n",
    "print(view.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the dataset\n",
    "\n",
    "Start browsing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_dashboard(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narrow your scope to 10 random samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.view = dataset.view().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on some some samples in the GUI to select them and access their IDs from\n",
    "code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the IDs of the currently selected samples in the dashboard\n",
    "sample_ids = session.selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a view that contains your currently selected samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_view = dataset.view().select(session.selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the dashboard to only show your selected samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.view = selected_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute file hashes\n",
    "\n",
    "Iterate over the samples and compute their file hashes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.core.utils as fou\n",
    "\n",
    "for sample in dataset:\n",
    "    sample[\"file_hash\"] = fou.compute_filehash(sample.filepath)\n",
    "    sample.save()\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two ways to visualize this new information:\n",
    "\n",
    "-   From your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset.view().first()\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   By refreshing the dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.dataset = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicates\n",
    "\n",
    "Now let's use a simple Python statement to locate the duplicate files in the\n",
    "dataset, i.e., those with the same file hashses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "filehash_counts = Counter(sample.file_hash for sample in dataset)\n",
    "dup_filehashes = [k for k, v in filehash_counts.items() if v > 1]\n",
    "\n",
    "print(\"Number of duplicate file hashes: %d\" % len(dup_filehashes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a view that contains only the samples with these duplicate\n",
    "file hashes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_view = (\n",
    "    dataset.view()\n",
    "    # Extract samples with duplicate file hashes\n",
    "    .match({\"file_hash\": {\"$in\": dup_filehashes}})\n",
    "    # Sort by file hash so duplicates will be adjacent\n",
    "    .sort_by(\"file_hash\")\n",
    ")\n",
    "\n",
    "print(\"Number of images that have a duplicate: %d\" % len(dup_view))\n",
    "print(\"Number of duplicates: %d\" % (len(dup_view) - len(dup_filehashes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can always use the dashboard to visualize our work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.view = dup_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete duplicates\n",
    "\n",
    "Now let's delete the duplicate samples from the dataset using our `dup_view` to\n",
    "restrict our attention to known duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of dataset before: %d\" % len(dataset))\n",
    "\n",
    "_dup_filehashes = set()\n",
    "for sample in dup_view:\n",
    "    if sample.file_hash not in _dup_filehashes:\n",
    "        _dup_filehashes.add(sample.file_hash)\n",
    "        continue\n",
    "\n",
    "    del dataset[sample.id]\n",
    "\n",
    "print(\"Length of dataset after: %d\" % len(dataset))\n",
    "\n",
    "# Verify that the dataset no longer contains any duplicates\n",
    "print(\"Number of unique file hashes: %d\" % len({s.file_hash for s in dataset}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the deduplicated dataset\n",
    "\n",
    "Finally, let's export a fresh copy of our now-duplicate-free dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_DIR = \"/tmp/fiftyone/export\"\n",
    "\n",
    "dataset.export(label_field=\"ground_truth\", export_dir=EXPORT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the contents of `/tmp/fiftyone/export` on disk to see how the data is\n",
    "organized.\n",
    "\n",
    "You can load the deduplicated dataset that you exported back into FiftyOne at\n",
    "any time as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dups_dataset = fo.Dataset.from_image_classification_dataset(\n",
    "    EXPORT_DIR, name=\"no_duplicates\"\n",
    ")\n",
    "\n",
    "print(no_dups_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "You can cleanup the files generated by this tutorial by running:\n",
    "\n",
    "```shell\n",
    "rm -rf /tmp/fiftyone\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "fiftyone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
