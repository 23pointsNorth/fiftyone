{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FiftyOne Walkthrough: Model Training with a Custom Model\n",
    "\n",
    "This walkthrough provides examples of how FiftyOne can be used during a model training procedure when your model definition code is custom.  It covers the following concepts:\n",
    "* Integrating your existing model-training loop with FiftyOne\n",
    "* Adding predictions from your model to your FiFtyOne dataset\n",
    "* Visualizing aspects of your dataset based on your newly trained model\n",
    "\n",
    "This walkthrough is self-contained and uses a custom model definition provided within."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "This code requires a Torch installation.  So, install it if necessary in your shell / virtual environment.\n",
    "\n",
    "```\n",
    "pip install torch\n",
    "pip install torchvision\n",
    "```\n",
    "XXX QUESTION: How self-contained should these be?  This requirement is already included in the README.\n",
    "\n",
    "Let's set up some basic variables and structures we need for the walkthrough.  Nothing in here is specific to FiftyOne; you probably don't need to understand it in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Settings; defaults are fine if you have a GPU.  Otherwise, you'll want to\n",
    "# reduce some values just to get the gist of the walkthrough\n",
    "settings = {}\n",
    "# These settings are for a powerful GPU with more than 6GBs Memory\n",
    "#settings['batch_size'] = 512\n",
    "# Use all of the samples for training\n",
    "#settings['take'] = None\n",
    "# These will work on GPU's with 4GB RAM\n",
    "# You may need to lower further to run the walkthrough\n",
    "settings['batch_size'] = 36\n",
    "# Use 10000 samples from the total training set\n",
    "settings['take'] = 10000  \n",
    "# 24 gets us to a good point in this setup\n",
    "settings['epochs'] = 24\n",
    "# Where to save the model\n",
    "settings['model_path'] = './model.pth'\n",
    "\n",
    "localtime = lambda: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "\n",
    "# Dataset Parameters\n",
    "# Set up the label map for the walkthrough.  We will be working with CIFAR-10.\n",
    "##  Dataset Setup\n",
    "cifar10_mean, cifar10_std = [\n",
    "    (125.31, 122.95, 113.87), \n",
    "    (62.99, 62.09, 66.70), \n",
    "]\n",
    "\n",
    "cifar10_map = \"airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\".split(', ')\n",
    "cifar10_rev = {name: index for index, name in enumerate(cifar10_map)}\n",
    "N_labels = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the data\n",
    "\n",
    "Let's use the CIFAR-10 dataset for simplicity.  And, let's get it from the FiftyOne zoo, without loss of generality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\"cifar10\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was pretty easy!  There are 60,000 samples loaded and we see tags of `train` and `test`.  Let's just make sure that things line up properly: there should be 50,000 samples for training and 10,000 for testing (or validation, as we will use them below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_view = dataset.view().match_tag(\"train\")\n",
    "print(\"train samples: %d\" % len(train_view))\n",
    "valid_view = dataset.view().match_tag(\"test\")\n",
    "print(\"validation samples: %d\" % len(valid_view))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache the data for training\n",
    "\n",
    "Next, as you may be aware, FiftyOne's `Dataset` object points to the actual data on disk and does not load it or cache it in any way, allowing FiftyOne to be fast and lightweight while giving you maximum flexibility.  So, since we are going to be training a model and this example data is small, let's cache all of the data in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio \n",
    "import numpy as np\n",
    "\n",
    "# Produces train_set and valid_set that are lists of tuples: (image, label)\n",
    "\n",
    "if train_view is None:\n",
    "    raise ValueError(\n",
    "        \"train expects 'train_view' in the global namespace. See README.md\"\n",
    "    )\n",
    "if valid_view is None:\n",
    "    raise ValueError(\n",
    "        \"train expects 'valid_view' in the global namespace. See README.md\"\n",
    "    )\n",
    "\n",
    "def update_progress(progress):\n",
    "    # progress is [0,1]\n",
    "    t = 51\n",
    "    i = int(progress*t)\n",
    "    r = t-i\n",
    "    print(\"\\r[%s%s] %.1f%%\" % (\"#\"*i, \" \"*r,  progress*100), end=\"\")\n",
    "    \n",
    "    \n",
    "if settings['take']:\n",
    "    train_view = train_view.take(settings['take'])\n",
    "    print(f\"using a subset of the data for the model training\")\n",
    "    print(f\"updated train set: {len(train_view)} samples\")\n",
    "\n",
    "    \n",
    "print(\"Training images\")\n",
    "_train_images = []\n",
    "_train_labels = []\n",
    "for index, sample in enumerate(train_view.iter_samples()):\n",
    "    image = np.array(imageio.imread(sample.filepath))\n",
    "    label = cifar10_rev[sample[\"ground_truth\"].label]\n",
    "    _train_images.append(image)\n",
    "    _train_labels.append(label)\n",
    "\n",
    "    if index % 100 == 0:\n",
    "        update_progress(index / len(train_view))\n",
    "update_progress(1)\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Validation images\")\n",
    "_valid_images = []\n",
    "_valid_labels = []\n",
    "for index, sample in enumerate(valid_view.iter_samples()):\n",
    "    image = np.array(imageio.imread(sample.filepath))\n",
    "    label = cifar10_rev[sample[\"ground_truth\"].label]\n",
    "    _valid_images.append(image)\n",
    "    _valid_labels.append(label)\n",
    "\n",
    "    if index % 100 == 0:\n",
    "        update_progress(index / len(valid_view))\n",
    "update_progress(1)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Using the model provided in `./simple_resnet.py`, let's now train a model and save it to disk.  This code uses Torch as the ML backend and implements a small resnet model.  We will train the model using the cached images from the FiftyOne dataset we made above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from functools import partial\n",
    "from simple_resnet import *\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, dataset, batch_size, shuffle, set_random_choices=False, num_workers=0, drop_last=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.set_random_choices = set_random_choices\n",
    "        self.dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=shuffle, drop_last=drop_last\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.set_random_choices:\n",
    "            self.dataset.set_random_choices()\n",
    "        return ({'input': x.to(device).half(), 'target': y.to(device).long()} for (x,y) in self.dataloader)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "# Set up the dataset structures for our model code and preprocess the data.\n",
    "whole_dataset = {\n",
    "    'train': {\n",
    "        'data': np.asarray(_train_images),\n",
    "        'targets': np.asarray(_train_labels)\n",
    "    },\n",
    "    'valid': {\n",
    "        'data': np.asarray(_valid_images),\n",
    "        'targets': np.asarray(_valid_labels)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Preprocessing training data\")\n",
    "transforms = [\n",
    "    partial(normalise, mean=np.array(cifar10_mean, dtype=np.float32), std=np.array(cifar10_std, dtype=np.float32)),\n",
    "    partial(transpose, source='NHWC', target='NCHW'),\n",
    "]\n",
    "train_set = list(zip(*preprocess(whole_dataset['train'], [partial(pad, border=4)] + transforms).values()))\n",
    "valid_set = list(zip(*preprocess(whole_dataset['valid'], transforms).values()))\n",
    "print(f\"Finished preprocessing\")\n",
    "\n",
    "print(f\"train set: {len(train_set)} samples\")\n",
    "print(f\"valid set: {len(valid_set)} samples\")\n",
    "\n",
    "\n",
    "# Set up the variables for training the model. \n",
    "lr_schedule = PiecewiseLinear([0, 5, settings['epochs']], [0, 0.4, 0])\n",
    "train_transforms = [Crop(32, 32), FlipLR(), Cutout(8, 8)]\n",
    "total_N = len(train_set)\n",
    "\n",
    "print(f'Starting the model training at {localtime()}')\n",
    "\n",
    "model = Network(simple_resnet()).to(device).half()\n",
    "logs, state = Table(), {MODEL: model, LOSS: x_ent_loss}\n",
    "\n",
    "valid_batches = DataLoader(valid_set, settings['batch_size'], shuffle=False, drop_last=False)\n",
    "\n",
    "train_batches = DataLoader(\n",
    "        Transform(train_set, train_transforms),\n",
    "        settings['batch_size'], shuffle=True, set_random_choices=True, drop_last=True\n",
    ")\n",
    "lr = lambda step: lr_schedule(step/len(train_batches))/settings['batch_size']\n",
    "opts = [\n",
    "    SGD(trainable_params(model).values(),\n",
    "    {'lr': lr, 'weight_decay': Const(5e-4*settings['batch_size']), 'momentum': Const(0.9)})\n",
    "]\n",
    "state[OPTS] = opts\n",
    "\n",
    "for epoch in range(settings['epochs']):\n",
    "    logs.append(union({'epoch': epoch+1}, train_epoch(state, Timer(torch.cuda.synchronize), train_batches, valid_batches)))\n",
    "logs.df().query(f'epoch=={settings[\"epochs\"]}')[['train_acc', 'valid_acc']].describe()\n",
    "\n",
    "if settings['model_path']:\n",
    "    torch.save(model.state_dict(),settings['model_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap-Up\n",
    "\n",
    "And, we're done!  If you used the GPU setup, you should see the `valid acc` field in the training log above reporting about 94% accuracy; if you used the lesser setup, your results will vary, but it should be around 85%. Well, this is really only the beginning of the journey. Here, we trained a toy model on the CIFAR-10 dataset with FiftyOne involved in the process.  \n",
    "\n",
    "XXX TODO Link to other walkthroughs in a sensible manner.  \n",
    "With this model, we could, for example, now begin to explore the training and validation dataset for uniqueness and possible label mistakes.\n",
    "\n",
    "In the sister-walkthrough, XXX LINK, we use Torch network definitions to directly train a model on a larger dataset.  \n",
    "\n",
    "XXX QUESTION: Should we incorporate some other ending here, such as visualization of predictions on the validation set, or the worst K predictions on the validation set?  Something to show the next stage value of the tool?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
